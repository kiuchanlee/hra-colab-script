
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime, timedelta
from tqdm import tqdm
import math

# âœ… êµ¬ê¸€ ì¸ì¦ ë° gspread ì„¤ì • (GitHub Actionsìš©)
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials

def authenticate_gspread():
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]
    creds = Credentials.from_service_account_file("creds.json", scopes=scopes)
    return gspread.authorize(creds)

gc = authenticate_gspread()

# âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news(query, category, start_date, end_date, max_page=1):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.naver.com"
    }

    media_codes = ",".join([
        "023", "025", "020", "032", "028", "015", "009",
        "081", "005", "008", "014", "016", "018", "277", "001"
    ])

    results = []
    seen_links = set()

    for start in range(1, max_page * 10 + 1, 10):
        print(f"\nğŸ“„ [{category} - {query}] í˜ì´ì§€ {((start - 1)//10) + 1} í¬ë¡¤ë§ ì¤‘...")

        url = (
            f"https://search.naver.com/search.naver?where=news&query={query}"
            f"&pd=4&ds={start_date}&de={end_date}&office_type=3&office_category=1"
            f"&sort=0&news_office_checked={media_codes}&start={start}"
        )

        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            page_links = list({a["href"] for a in soup.select("a.info") if "n.news.naver.com" in a["href"]})

            print(f"ğŸ”— ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ ìˆ˜: {len(page_links)}")

            for link in page_links:
                if link in seen_links:
                    continue
                seen_links.add(link)

                try:
                    article = requests.get(link, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article.text, "html.parser")

                    content = article_soup.select_one("div#newsct_article")
                    content_text = content.get_text(separator=" ").strip() if content else ""

                    title_tag = article_soup.select_one("h2#title_area span")
                    title = title_tag.get_text().strip() if title_tag else ""

                    press_tag = article_soup.select_one("img.media_end_head_top_logo_img")
                    press = press_tag['alt'].strip() if press_tag and 'alt' in press_tag.attrs else ""

                    date_tag = article_soup.select_one("span.media_end_head_info_datestamp_time")
                    raw_date = date_tag.get_text().strip() if date_tag else ""

                    try:
                        raw_date_fixed = raw_date.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM")
                        dt = datetime.strptime(raw_date_fixed, "%Y.%m.%d. %p %I:%M")
                        weekday_kor = ["ì¼", "ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† "][(dt.weekday() + 1) % 7]
                        formatted_date = dt.strftime(f"%m.%d({weekday_kor})")
                    except Exception:
                        formatted_date = raw_date

                    results.append({
                        "êµ¬ë¶„": category,
                        "í‚¤ì›Œë“œ": query,
                        "ì¼ì": formatted_date,
                        "í—¤ë“œë¼ì¸": title,
                        "ë³¸ë¬¸": content_text,
                        "ë§¤ì²´ëª…": press,
                        "URL": link
                    })

                    print(f"âœ… [{formatted_date}] [{press}] {title[:30]}...")
                    time.sleep(1)

                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {link} - {e}")
                    continue

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

    return pd.DataFrame(results)

# âœ… í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜
keywordGroups = [
    {
        "category": "HR",
        "keywords": ["ì¡°ì§ê°œí¸", "ì¸ì‚¬ì œë„", "ì§ë¬´ì¤‘ì‹¬ ì¸ì‚¬", "ì§ë¬´ê¸‰ì œ", "ì„±ê³¼ê¸‰ì œ", "í˜¸ë´‰ì œ ê°œí¸", "ì§ê¸‰ì²´ê³„",
                     "ì±„ìš©ê³µê³ ", "ì‹ ì…ê³µì±„", "ì»¬ì²˜í•", "ì»¬ì³í•", "ì‹ ì…ì±„ìš©", "ê²½ë ¥ì±„ìš©", "ì¸í„´ì±„ìš©",
                     "ì±„ìš©ì‹œì¥", "ë¸”ë¼ì¸ë“œì±„ìš©", "ì±„ìš©ê³µì •ì„±", "ê³ ìš©ì‹œì¥", "ê²½ë ¥ë‹¨ì ˆ", "ì±„ìš©ë©´ì ‘",
                     "ê³µì±„ íì§€", "ìƒì‹œì±„ìš©", "ì¸ì¬ í™•ë³´", "ë¦¬í¬ë£¨íŒ…", "ì±„ìš© ì „ëµ", "ìˆ˜ì‹œì±„ìš©", "ê¸€ë¡œë²Œ ì¸ì¬",
                     "ì¸ì‚¬í‰ê°€", "ì„±ê³¼í‰ê°€", "ì—­ëŸ‰í‰ê°€", "ë³´ìƒì²´ê³„", "ì—°ë´‰ì œ", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ",
                     "ì„ê¸ˆì¸ìƒ", "ì„ê¸ˆí˜‘ìƒ", "ê¸°ë³¸ê¸‰", "ì—°ì°¨ìˆ˜ë‹¹", "í†µìƒì„ê¸ˆ", "ì„ê¸ˆí”¼í¬",
                     "ì„ê¸ˆì²´ê³„", "ì‹œê¸‰ì œ", "ìµœì €ì„ê¸ˆ", "ê³ ì •ê¸‰", "ì„±ê³¼ì—°ë´‰", "í‡´ì§ê¸ˆ",
                     "í‰ê· ì—°ë´‰", "ê·¼ì†", "í¬ê´„ì„ê¸ˆ", "ì‹¤ì—…ê¸‰ì—¬", "ì—°ê¸ˆê°œí˜", "ìŠ¤í†¡ì˜µì…˜", "RSU",
                     "ì‚¬ë‚´ êµìœ¡", "ë¦¬ìŠ¤í‚¬ë§", "ì—…ìŠ¤í‚¬ë§", "êµìœ¡í›ˆë ¨", "HRD", "ì‚¬ë‚´ëŒ€í•™", "ì¡í¬ìŠ¤íŒ…", "ì‚¬ë‚´ ì–‘ì„±",
                     "ì»¤ë¦¬ì–´íŒ¨ìŠ¤", "ê²½ë ¥ê°œë°œ", "ì§ë¬´ìˆœí™˜", "ìŠ¹ì§„ì œë„", "ë¦¬ë”ì‹­ í”„ë¡œê·¸ë¨", "í›„ê³„ì ì–‘ì„±", "ì „ì§",
                     "52ì‹œê°„", "ìœ ì—°ê·¼ë¡œ", "ì¬íƒê·¼ë¬´", "ìœ ì—°ê·¼ë¬´ì œ", "ì„ íƒê·¼ë¡œì œ", "ê·¼ë¡œì‹œê°„ ë‹¨ì¶•", "ì£¼4ì¼ì œ", "ë‹¨ì¶•ê·¼ë¡œ", "ê·¼ë¬´ì‹œê°„", "ìœ¡ì•„íœ´ì§", "íœ´ì§ì œë„", "ì¶œì‚°íœ´ê°€", "ê·¼ì†íœ´ê°€", "ì‚¬ë‚´ë³µì§€",
                     "ì¡°ì§ë¬¸í™”", "ì›Œë¼ë°¸", "DEI", "ì‚¬ë‚´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
                     "ì¸ì‚¬ ì •ì±…", "ê³ ìš© ì •ì±…", "ë…¸ë™ì‹œì¥ ê°œí¸", "ì¸ì‚¬ í–‰ì •", "ì¸ì¬ ì •ì±…",
                     "ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ë²•", "ë…¸ë™ì‹œê°„", "ì¤‘ëŒ€ì¬í•´ë²•",
                     "ì§ì¥ ë‚´ ê´´ë¡­í˜", "ê³ ìš© ì•ˆì •ì„±", "ì‚°ì—…ì¬í•´", "ë…¸ì¡°í˜‘ì•½", "ì„ë‹¨í˜‘", "ë‹¨ì²´êµì„­",
                     "ê·¼ë¡œí™˜ê²½", "ê·¼ë¡œê³„ì•½", "ì •ë…„ì—°ì¥", "íŒŒì—…", "ì‹¤ì—…ì", "ë…¸ì¡°í™œë™",
                     "ë…¸ì‚¬", "ë…¸ì‚¬ê°ˆë“±", "êµ¬ì¡°ì¡°ì •", "ì„ê¸ˆí˜‘ìƒ", "í¬ë§í‡´ì§",
                     "HR Tech", "AI ì±„ìš©", "HR ìë™í™”", "ì¸ì‚¬ê´€ë¦¬ ì‹œìŠ¤í…œ", "HRIS",
                     "ì¸ì‚¬ ë°ì´í„°", "HR Analytics", "ì¸ì‚¬ ë°ì´í„° ë¶„ì„", "ë°ì´í„° ê¸°ë°˜ í‰ê°€",
                     "ì‚¼ì„± ì¸ì‚¬", "ì‚¼ì„± ì„ì›ì§„", "ì‚¼ì„± ì¡°ì§", "ì‚¼ì„± ê²½ì˜ì§„", "ì‚¼ì„± ì¡°ì§ê°œí¸", "ì‚¼ì„± ì˜ì…", "ì‚¼ì„± ë‚´ì •", "ì‚¼ì„± ë°œë ¹", "ì‚¼ì„± ì´ì‚¬",
                     "í˜„ëŒ€í•´ìƒ ì¸ì‚¬", "í˜„ëŒ€í•´ìƒ ì„ì›ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§", "í˜„ëŒ€í•´ìƒ ê²½ì˜ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§ê°œí¸", "í˜„ëŒ€í•´ìƒ ì˜ì…", "í˜„ëŒ€í•´ìƒ ë‚´ì •", "í˜„ëŒ€í•´ìƒ ë°œë ¹", "í˜„ëŒ€í•´ìƒ ì´ì‚¬",
                     "DBì†ë³´ ì¸ì‚¬", "DBì†ë³´ ì„ì›ì§„", "DBì†ë³´ ì¡°ì§", "DBì†ë³´ ê²½ì˜ì§„", "DBì†ë³´ ì¡°ì§ê°œí¸", "DBì†ë³´ ì˜ì…", "DBì†ë³´ ë‚´ì •", "DBì†ë³´ ë°œë ¹", "DBì†ë³´ ì´ì‚¬",
                     "KBì†ë³´ ì¸ì‚¬", "KBì†ë³´ ì„ì›ì§„", "KBì†ë³´ ì¡°ì§", "KBì†ë³´ ê²½ì˜ì§„", "KBì†ë³´ ì¡°ì§ê°œí¸", "KBì†ë³´ ì˜ì…", "KBì†ë³´ ë‚´ì •", "KBì†ë³´ ë°œë ¹", "KBì†ë³´ ì´ì‚¬",
                     "ë©”ë¦¬ì¸  ì¸ì‚¬", "ë©”ë¦¬ì¸  ì„ì›ì§„", "ë©”ë¦¬ì¸  ì¡°ì§", "ë©”ë¦¬ì¸  ê²½ì˜ì§„", "ë©”ë¦¬ì¸  ì¡°ì§ê°œí¸", "ë©”ë¦¬ì¸  ì˜ì…", "ë©”ë¦¬ì¸  ë‚´ì •", "ë©”ë¦¬ì¸  ë°œë ¹", "ë©”ë¦¬ì¸  ì´ì‚¬"]
    }
]

# âœ… ë‚ ì§œ ì„¤ì •
print("\nğŸ“† ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •")
days_ago = 1
end_date = datetime.today().strftime("%Y.%m.%d")
start_date = (datetime.today() - timedelta(days=days_ago)).strftime("%Y.%m.%d")
print(f"ğŸ“† ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n")

# âœ… ì „ì²´ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹¤í–‰
all_results = []
for group in keywordGroups:
    for keyword in group["keywords"]:
        df = crawl_news(keyword, group["category"], start_date, end_date, max_page=1)
        all_results.append(df)



# âœ… ë°ì´í„° ì •ë¦¬
df_total = pd.concat(all_results, ignore_index=True)
df_total = df_total.drop_duplicates(subset=["URL"])
df_total = df_total[["êµ¬ë¶„", "í‚¤ì›Œë“œ", "ì¼ì", "í—¤ë“œë¼ì¸", "ë³¸ë¬¸", "ë§¤ì²´ëª…", "URL"]]
df_total['í—¤ë“œë¼ì¸'] = df_total['í—¤ë“œë¼ì¸'].str.replace(r"\[.*?\]", "", regex=True).str.strip()
df_total = df_total.sort_values(by=["êµ¬ë¶„", "ì¼ì", "í—¤ë“œë¼ì¸"], ascending=[True, False, True])

df_total.to_csv("crawled_news.csv", index=False)
print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ ë° csv ì €ì¥!")
