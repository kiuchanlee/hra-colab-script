# -*- coding: utf-8 -*-
"""hra_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xm9AQCM3P8qThV9HMpPRiIqBgEraKZ1h
"""

#콜랍 수정본
# -*- coding: utf-8 -*-
"""hra_news.ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1Xm9AQCM3P8qThV9HMpPRiIqBgEraKZ1h
"""
#콜랩 수정본
# ✅ 1. 라이브러리 설치 - 주석 처리하거나 제거

# ✅ 2. 기본 라이브러리
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta

# ✅ 3. 구글 인증 및 gspread 설정 - GitHub Actions용으로 수정
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials

# 서비스 계정을 사용한 인증 방식으로 변경
def authenticate_gspread():
    # GitHub Secrets에서 설정한 서비스 계정 키 파일 사용
    scope = ['https://spreadsheets.google.com/feeds',
             'https://www.googleapis.com/auth/drive']
    
    # 서비스 계정 JSON 파일 경로
    creds = Credentials.from_service_account_file('service-account.json', scopes=scope)
    return gspread.authorize(creds)

gc = authenticate_gspread()

# ✅ 4. 뉴스 크롤링 함수
def crawl_news(query, category, start_date, end_date, max_page=1):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.naver.com"
    }

    media_codes = ",".join([
        "1000001",  # 조선일보
        "1000002",  # 중앙일보
        "1000003",  # 동아일보
        "1000005",  # 한국경제
        "1000006",  # 매일경제
        "1000009",  # 서울신문
        "1000011",  # 국민일보
        "1000013",  # 머니투데이
        "1000014",  # 파이낸셜뉴스
        "1000016",  # 헤럴드경제
        "1000017",  # 이데일리
        "1000018",  # 아시아경제
        "1010001",  # 연합뉴스
    ])

    results = []
    seen_links = set()

    for start in range(1, max_page * 10 + 1, 10):
        print(f"\n📄 [{category} - {query}] 페이지 {((start - 1)//10) + 1} 크롤링 중...")

        url = (
            f"https://search.naver.com/search.naver?where=news&query={query}"
            f"&pd=4&ds={start_date}&de={end_date}&office_type=3&office_category=1"
            f"&sort=0&news_office_checked={media_codes}&start={start}"
        )

        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            page_links = list({a["href"] for a in soup.select("a.info") if "n.news.naver.com" in a["href"]})

            print(f"🔗 수집된 기사 링크 수: {len(page_links)}")

            for link in page_links:
                if link in seen_links:
                    continue
                seen_links.add(link)

                try:
                    article = requests.get(link, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article.text, "html.parser")

                    content = article_soup.select_one("div#newsct_article")
                    content_text = content.get_text(separator=" ").strip() if content else ""

                    title_tag = article_soup.select_one("h2#title_area span")
                    title = title_tag.get_text().strip() if title_tag else ""

                    press_tag = article_soup.select_one("img.media_end_head_top_logo_img")
                    press = press_tag['alt'].strip() if press_tag and 'alt' in press_tag.attrs else ""

                    date_tag = article_soup.select_one("span.media_end_head_info_datestamp_time")
                    raw_date = date_tag.get_text().strip() if date_tag else ""

                    try:
                       dt = datetime.strptime(raw_date, "%Y.%m.%d. %p %I:%M")
                       # 일요일=0, 월~토=1~6
                       weekday_kor = ["일", "월", "화", "수", "목", "금", "토"][(dt.weekday() + 1) % 7]
                       formatted_date = dt.strftime(f"%m.%d({weekday_kor})")
                    except:
                       formatted_date = raw_date


                    results.append({
                        "구분": category,
                        "키워드": query,
                        "일자": formatted_date,
                        "헤드라인": title,
                        "본문": content_text,
                        "매체명": press,
                        "URL": link
                    })

                    print(f"✅ [{formatted_date}] [{press}] {title[:30]}...")

                    time.sleep(1)

                except Exception as e:
                    print(f"⚠️ 기사 수집 실패: {link} - {e}")
                    continue

        except Exception as e:
            print(f"❌ 페이지 요청 실패: {e}")
            continue

    return pd.DataFrame(results)

# ✅ 5. 키워드 그룹 정의
keywordGroups = [
  { "category": "채용", "keywords": ["채용공고", "대기업공채", "컬처핏", "컬쳐핏", "신입채용", "경력채용", "인턴채용", "채용시장", "블라인드채용", "채용공정성", "고용시장", "경력단절", "채용면접", "워라밸"], "maxResults": 20 },
  { "category": "노사", "keywords": ["노조협약", "임단협", "근로기준법", "노동법", "임금협상", "단체교섭", "근로환경", "희망퇴직", "노사", "산업안전보건법", "파업", "실업자", "노조활동", "근로계약", "노동시장", "임금체불", "고용안정", "산업재해", "정년연장", "구조조정", "노사갈등"], "maxResults": 20 },
  { "category": "임금", "keywords": ["임금인상", "성과급", "임금협상", "연금개혁", "기본급", "연차수당", "통상임금", "임금피크", "임금체계", "시급제", "최저임금", "고정급", "성과연봉", "퇴직금", "평균연봉", "근속", "포괄임금", "실업급여"], "maxResults": 20 },
  { "category": "제도", "keywords": ["52시간", "유연근로", "재택근무", "건강보험료", "고용보험", "퇴직연금제도", "단축근로", "근무시간", "육아휴직", "출산휴가", "모성보호"], "maxResults": 20 },
  { "category": "복지", "keywords": ["사내복지", "출산휴가", "건강검진", "경조사지원", "복지포인트", "직장어린이집", "사내식당", "근속포상", "복리후생", "사택", "사내동아리", "구내식당"], "maxResults": 20 }
  { "category": "관계사", "keywords": ["삼성 임원진", "삼성 조직", "삼성 경영진", "삼성 조직개편", "삼성 영입", "삼성 내정", "삼성 발령", "삼성 이사"], "maxResults": 20 }
  { "category": "현대해상", "keywords": ["현대해상 임원진", "현대해상 조직", "현대해상 경영진", "현대해상 조직개편", "현대해상 영입", "현대해상 내정", "현대해상 발령", "현대해상 이사"], "maxResults": 20 }
  { "category": "DB손보", "keywords": ["DB손보 임원진", "DB손보 조직", "DB손보 경영진", "DB손보 조직개편", "DB손보 영입", "DB손보 내정", "DB손보 발령", "DB손보 이사"], "maxResults": 20 }
{ "category": "KB손보", "keywords": ["KB손보 임원진", "KB손보 조직", "KB손보 경영진", "KB손보 조직개편", "KB손보 영입", "KB손보 내정", "KB손보 발령", "KB손보 이사"], "maxResults": 20 }
{ "category": "메리츠", "keywords": ["메리츠 임원진", "메리츠 조직", "메리츠 경영진", "메리츠 조직개편", "메리츠 영입", "메리츠 내정", "메리츠 발령", "메리츠 이사"], "maxResults": 20 }



]

# ✅ 6. 날짜 설정
days_ago = 1
end_date = datetime.today().strftime("%Y.%m.%d")
start_date = (datetime.today() - timedelta(days=days_ago)).strftime("%Y.%m.%d")
print(f"\n📆 수집 기간: {start_date} ~ {end_date}\n")

# ✅ 7. 전체 크롤링 실행
all_results = []

for group in keywordGroups:
    for keyword in group["keywords"]:
        df = crawl_news(keyword, group["category"], start_date, end_date, max_page=1)
        all_results.append(df)

# ✅ 8. 정리
df_total = pd.concat(all_results, ignore_index=True)
df_total = df_total.drop_duplicates(subset=["헤드라인"])
df_total = df_total[["구분", "키워드", "일자", "헤드라인", "본문", "매체명", "URL"]]
df_total = df_total.sort_values(by=["구분", "일자", "헤드라인"], ascending=[True, False, True])

# ✅ 9. Google 시트 업로드 (덮어쓰기)
sheet_id = "1OEUq2ZCt0WeZv3aUTYaqcy91YO-05LtUjhXJj0GRRyg"
sheet_name = "네이버크롤링2"

sheet = gc.open_by_key(sheet_id)
worksheet = sheet.worksheet(sheet_name)
worksheet.clear()
set_with_dataframe(worksheet, df_total)

print("✅ Google 스프레드시트 저장 완료!")
print("🔗 링크:", sheet.url)
