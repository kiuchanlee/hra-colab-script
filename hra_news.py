# -*- coding: utf-8 -*-
"""hra_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xm9AQCM3P8qThV9HMpPRiIqBgEraKZ1h
"""

#ì½œë ìˆ˜ì •ë³¸
# -*- coding: utf-8 -*-
"""hra_news.ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1Xm9AQCM3P8qThV9HMpPRiIqBgEraKZ1h
"""
#ì½œë© ìˆ˜ì •ë³¸
# âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ - ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°

# âœ… 2. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta

# âœ… 3. êµ¬ê¸€ ì¸ì¦ ë° gspread ì„¤ì • - GitHub Actionsìš©ìœ¼ë¡œ ìˆ˜ì •
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials

# ì„œë¹„ìŠ¤ ê³„ì •ì„ ì‚¬ìš©í•œ ì¸ì¦ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
def authenticate_gspread():
    # GitHub Secretsì—ì„œ ì„¤ì •í•œ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì‚¬ìš©
    scope = ['https://spreadsheets.google.com/feeds',
             'https://www.googleapis.com/auth/drive']
    
    # ì„œë¹„ìŠ¤ ê³„ì • JSON íŒŒì¼ ê²½ë¡œ
    creds = Credentials.from_service_account_file('service-account.json', scopes=scope)
    return gspread.authorize(creds)

gc = authenticate_gspread()

# âœ… 4. ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news(query, category, start_date, end_date, max_page=1):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.naver.com"
    }

    media_codes = ",".join([
        "1000001",  # ì¡°ì„ ì¼ë³´
        "1000002",  # ì¤‘ì•™ì¼ë³´
        "1000003",  # ë™ì•„ì¼ë³´
        "1000005",  # í•œêµ­ê²½ì œ
        "1000006",  # ë§¤ì¼ê²½ì œ
        "1000009",  # ì„œìš¸ì‹ ë¬¸
        "1000011",  # êµ­ë¯¼ì¼ë³´
        "1000013",  # ë¨¸ë‹ˆíˆ¬ë°ì´
        "1000014",  # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
        "1000016",  # í—¤ëŸ´ë“œê²½ì œ
        "1000017",  # ì´ë°ì¼ë¦¬
        "1000018",  # ì•„ì‹œì•„ê²½ì œ
        "1010001",  # ì—°í•©ë‰´ìŠ¤
    ])

    results = []
    seen_links = set()

    for start in range(1, max_page * 10 + 1, 10):
        print(f"\nğŸ“„ [{category} - {query}] í˜ì´ì§€ {((start - 1)//10) + 1} í¬ë¡¤ë§ ì¤‘...")

        url = (
            f"https://search.naver.com/search.naver?where=news&query={query}"
            f"&pd=4&ds={start_date}&de={end_date}&office_type=3&office_category=1"
            f"&sort=0&news_office_checked={media_codes}&start={start}"
        )

        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            page_links = list({a["href"] for a in soup.select("a.info") if "n.news.naver.com" in a["href"]})

            print(f"ğŸ”— ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ ìˆ˜: {len(page_links)}")

            for link in page_links:
                if link in seen_links:
                    continue
                seen_links.add(link)

                try:
                    article = requests.get(link, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article.text, "html.parser")

                    content = article_soup.select_one("div#newsct_article")
                    content_text = content.get_text(separator=" ").strip() if content else ""

                    title_tag = article_soup.select_one("h2#title_area span")
                    title = title_tag.get_text().strip() if title_tag else ""

                    press_tag = article_soup.select_one("img.media_end_head_top_logo_img")
                    press = press_tag['alt'].strip() if press_tag and 'alt' in press_tag.attrs else ""

                    date_tag = article_soup.select_one("span.media_end_head_info_datestamp_time")
                    raw_date = date_tag.get_text().strip() if date_tag else ""

                    try:
                       dt = datetime.strptime(raw_date, "%Y.%m.%d. %p %I:%M")
                       # ì¼ìš”ì¼=0, ì›”~í† =1~6
                       weekday_kor = ["ì¼", "ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† "][(dt.weekday() + 1) % 7]
                       formatted_date = dt.strftime(f"%m.%d({weekday_kor})")
                    except:
                       formatted_date = raw_date


                    results.append({
                        "êµ¬ë¶„": category,
                        "í‚¤ì›Œë“œ": query,
                        "ì¼ì": formatted_date,
                        "í—¤ë“œë¼ì¸": title,
                        "ë³¸ë¬¸": content_text,
                        "ë§¤ì²´ëª…": press,
                        "URL": link
                    })

                    print(f"âœ… [{formatted_date}] [{press}] {title[:30]}...")

                    time.sleep(1)

                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {link} - {e}")
                    continue

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

    return pd.DataFrame(results)

# âœ… 5. í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜
keywordGroups = [
  { "category": "ì±„ìš©", "keywords": ["ì±„ìš©ê³µê³ ", "ëŒ€ê¸°ì—…ê³µì±„", "ì»¬ì²˜í•", "ì»¬ì³í•", "ì‹ ì…ì±„ìš©", "ê²½ë ¥ì±„ìš©", "ì¸í„´ì±„ìš©", "ì±„ìš©ì‹œì¥", "ë¸”ë¼ì¸ë“œì±„ìš©", "ì±„ìš©ê³µì •ì„±", "ê³ ìš©ì‹œì¥", "ê²½ë ¥ë‹¨ì ˆ", "ì±„ìš©ë©´ì ‘", "ì›Œë¼ë°¸"], "maxResults": 20 },
  { "category": "ë…¸ì‚¬", "keywords": ["ë…¸ì¡°í˜‘ì•½", "ì„ë‹¨í˜‘", "ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ë²•", "ì„ê¸ˆí˜‘ìƒ", "ë‹¨ì²´êµì„­", "ê·¼ë¡œí™˜ê²½", "í¬ë§í‡´ì§", "ë…¸ì‚¬", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•", "íŒŒì—…", "ì‹¤ì—…ì", "ë…¸ì¡°í™œë™", "ê·¼ë¡œê³„ì•½", "ë…¸ë™ì‹œì¥", "ì„ê¸ˆì²´ë¶ˆ", "ê³ ìš©ì•ˆì •", "ì‚°ì—…ì¬í•´", "ì •ë…„ì—°ì¥", "êµ¬ì¡°ì¡°ì •", "ë…¸ì‚¬ê°ˆë“±"], "maxResults": 20 },
  { "category": "ì„ê¸ˆ", "keywords": ["ì„ê¸ˆì¸ìƒ", "ì„±ê³¼ê¸‰", "ì„ê¸ˆí˜‘ìƒ", "ì—°ê¸ˆê°œí˜", "ê¸°ë³¸ê¸‰", "ì—°ì°¨ìˆ˜ë‹¹", "í†µìƒì„ê¸ˆ", "ì„ê¸ˆí”¼í¬", "ì„ê¸ˆì²´ê³„", "ì‹œê¸‰ì œ", "ìµœì €ì„ê¸ˆ", "ê³ ì •ê¸‰", "ì„±ê³¼ì—°ë´‰", "í‡´ì§ê¸ˆ", "í‰ê· ì—°ë´‰", "ê·¼ì†", "í¬ê´„ì„ê¸ˆ", "ì‹¤ì—…ê¸‰ì—¬"], "maxResults": 20 },
  { "category": "ì œë„", "keywords": ["52ì‹œê°„", "ìœ ì—°ê·¼ë¡œ", "ì¬íƒê·¼ë¬´", "ê±´ê°•ë³´í—˜ë£Œ", "ê³ ìš©ë³´í—˜", "í‡´ì§ì—°ê¸ˆì œë„", "ë‹¨ì¶•ê·¼ë¡œ", "ê·¼ë¬´ì‹œê°„", "ìœ¡ì•„íœ´ì§", "ì¶œì‚°íœ´ê°€", "ëª¨ì„±ë³´í˜¸"], "maxResults": 20 },
  { "category": "ë³µì§€", "keywords": ["ì‚¬ë‚´ë³µì§€", "ì¶œì‚°íœ´ê°€", "ê±´ê°•ê²€ì§„", "ê²½ì¡°ì‚¬ì§€ì›", "ë³µì§€í¬ì¸íŠ¸", "ì§ì¥ì–´ë¦°ì´ì§‘", "ì‚¬ë‚´ì‹ë‹¹", "ê·¼ì†í¬ìƒ", "ë³µë¦¬í›„ìƒ", "ì‚¬íƒ", "ì‚¬ë‚´ë™ì•„ë¦¬", "êµ¬ë‚´ì‹ë‹¹"], "maxResults": 20 }
  { "category": "ê´€ê³„ì‚¬", "keywords": ["ì‚¼ì„± ì„ì›ì§„", "ì‚¼ì„± ì¡°ì§", "ì‚¼ì„± ê²½ì˜ì§„", "ì‚¼ì„± ì¡°ì§ê°œí¸", "ì‚¼ì„± ì˜ì…", "ì‚¼ì„± ë‚´ì •", "ì‚¼ì„± ë°œë ¹", "ì‚¼ì„± ì´ì‚¬"], "maxResults": 20 }
  { "category": "í˜„ëŒ€í•´ìƒ", "keywords": ["í˜„ëŒ€í•´ìƒ ì„ì›ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§", "í˜„ëŒ€í•´ìƒ ê²½ì˜ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§ê°œí¸", "í˜„ëŒ€í•´ìƒ ì˜ì…", "í˜„ëŒ€í•´ìƒ ë‚´ì •", "í˜„ëŒ€í•´ìƒ ë°œë ¹", "í˜„ëŒ€í•´ìƒ ì´ì‚¬"], "maxResults": 20 }
  { "category": "DBì†ë³´", "keywords": ["DBì†ë³´ ì„ì›ì§„", "DBì†ë³´ ì¡°ì§", "DBì†ë³´ ê²½ì˜ì§„", "DBì†ë³´ ì¡°ì§ê°œí¸", "DBì†ë³´ ì˜ì…", "DBì†ë³´ ë‚´ì •", "DBì†ë³´ ë°œë ¹", "DBì†ë³´ ì´ì‚¬"], "maxResults": 20 }
{ "category": "KBì†ë³´", "keywords": ["KBì†ë³´ ì„ì›ì§„", "KBì†ë³´ ì¡°ì§", "KBì†ë³´ ê²½ì˜ì§„", "KBì†ë³´ ì¡°ì§ê°œí¸", "KBì†ë³´ ì˜ì…", "KBì†ë³´ ë‚´ì •", "KBì†ë³´ ë°œë ¹", "KBì†ë³´ ì´ì‚¬"], "maxResults": 20 }
{ "category": "ë©”ë¦¬ì¸ ", "keywords": ["ë©”ë¦¬ì¸  ì„ì›ì§„", "ë©”ë¦¬ì¸  ì¡°ì§", "ë©”ë¦¬ì¸  ê²½ì˜ì§„", "ë©”ë¦¬ì¸  ì¡°ì§ê°œí¸", "ë©”ë¦¬ì¸  ì˜ì…", "ë©”ë¦¬ì¸  ë‚´ì •", "ë©”ë¦¬ì¸  ë°œë ¹", "ë©”ë¦¬ì¸  ì´ì‚¬"], "maxResults": 20 }



]

# âœ… 6. ë‚ ì§œ ì„¤ì •
days_ago = 1
end_date = datetime.today().strftime("%Y.%m.%d")
start_date = (datetime.today() - timedelta(days=days_ago)).strftime("%Y.%m.%d")
print(f"\nğŸ“† ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n")

# âœ… 7. ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
all_results = []

for group in keywordGroups:
    for keyword in group["keywords"]:
        df = crawl_news(keyword, group["category"], start_date, end_date, max_page=1)
        all_results.append(df)

# âœ… 8. ì •ë¦¬
df_total = pd.concat(all_results, ignore_index=True)
df_total = df_total.drop_duplicates(subset=["í—¤ë“œë¼ì¸"])
df_total = df_total[["êµ¬ë¶„", "í‚¤ì›Œë“œ", "ì¼ì", "í—¤ë“œë¼ì¸", "ë³¸ë¬¸", "ë§¤ì²´ëª…", "URL"]]
df_total = df_total.sort_values(by=["êµ¬ë¶„", "ì¼ì", "í—¤ë“œë¼ì¸"], ascending=[True, False, True])

# âœ… 9. Google ì‹œíŠ¸ ì—…ë¡œë“œ (ë®ì–´ì“°ê¸°)
sheet_id = "1OEUq2ZCt0WeZv3aUTYaqcy91YO-05LtUjhXJj0GRRyg"
sheet_name = "ë„¤ì´ë²„í¬ë¡¤ë§2"

sheet = gc.open_by_key(sheet_id)
worksheet = sheet.worksheet(sheet_name)
worksheet.clear()
set_with_dataframe(worksheet, df_total)

print("âœ… Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì €ì¥ ì™„ë£Œ!")
print("ğŸ”— ë§í¬:", sheet.url)
