import urllib.request
import urllib.parse
import json
import pandas as pd
from datetime import datetime, timedelta
import html  # íŠ¹ìˆ˜ë¬¸ì ì œê±°ìš©
import os

def search_naver_news_multi(queries, client_id, client_secret, display=300, filter_press_names=[]):
    all_results = []

    # ê¸°ì¤€ ë‚ ì§œ: ì˜¤ëŠ˜ ê¸°ì¤€ 1ì¼ ì „
    start = (datetime.now() - timedelta(days=1)).date()

    for query in queries:
        print(f"\nğŸ” ê²€ìƒ‰ì–´ '{query}' ì²˜ë¦¬ ì¤‘...")

        query_results = []

        for start_index in range(1, min(display, 1000)+1, 100):
            encText = urllib.parse.quote(query)
            url = f"https://openapi.naver.com/v1/search/news?query={encText}&display=100&start={start_index}&sort=date"

            request = urllib.request.Request(url)
            request.add_header("X-Naver-Client-Id", client_id)
            request.add_header("X-Naver-Client-Secret", client_secret)

            try:
                response = urllib.request.urlopen(request)
            except Exception as e:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e} (query: '{query}')")
                continue

            if response.getcode() != 200:
                print(f"âŒ Error Code: {response.getcode()} for query '{query}'")
                continue

            response_body = response.read().decode('utf-8')
            news_data = json.loads(response_body)

            for item in news_data['items']:
                title = html.unescape(item['title'].replace('<b>', '').replace('</b>', ''))
                description = html.unescape(item['description'].replace('<b>', '').replace('</b>', ''))
                link = item['link']
                pubDate = datetime.strptime(item['pubDate'], "%a, %d %b %Y %H:%M:%S %z")

                # ì•ˆì „í•œ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì¶”ì¶œ
                if 'originallink' in item:
                    parts = item['originallink'].split('/')
                    press_name = parts[2] if len(parts) > 2 else ''
                else:
                    parts = item['link'].split('/')
                    press_name = parts[2] if len(parts) > 2 else ''

                # ë‚ ì§œ í•„í„° (ì–´ì œ ì´í›„)
                if pubDate.date() < start:
                    continue

                # ì–¸ë¡ ì‚¬ í•„í„°
                if filter_press_names and not any(name in press_name for name in filter_press_names):
                    continue

                query_results.append({
                    "ê²€ìƒ‰ì–´": query,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": description,
                    "ë‚ ì§œ": pubDate.strftime("%Y-%m-%d"),
                    "ì–¸ë¡ ì‚¬": press_name
                })

        print(f"âœ… '{query}' ì™„ë£Œ - {len(query_results)}ê±´ ìˆ˜ì§‘ë¨")
        all_results.extend(query_results)

    df = pd.DataFrame(all_results)

    # âœ… ì¤‘ë³µ URL ì œê±°
    df = df.drop_duplicates(subset=["URL"])

    # âœ… URL í•„í„°ë§ (ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ URLë§Œ)
    df = df[df["URL"].str.startswith("https://n.news.naver.com/mnews")].reset_index(drop=True)

    return df

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ì½ê¸°
    queries_str = os.getenv("QUERIES", "")
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not queries_str or not client_id or not client_secret:
        print("âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (QUERIES, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)")
        exit(1)

    queries = [q.strip() for q in queries_str.split(",") if q.strip()]
    
    df = search_naver_news_multi(
        queries=queries,
        client_id=client_id,
        client_secret=client_secret,
        display=300,
        filter_press_names=
            [
            "chosun.com",         # ì¡°ì„ ì¼ë³´
            "joins.com",          # ì¤‘ì•™ì¼ë³´
            "donga.com",          # ë™ì•„ì¼ë³´
            "khan.co.kr",         # ê²½í–¥ì‹ ë¬¸
            "hani.co.kr",         # í•œê²¨ë ˆ
            "hankyung.com",       # í•œêµ­ê²½ì œ
            "mk.co.kr",           # ë§¤ì¼ê²½ì œ
            "hankookilbo.com",    # í•œêµ­ì¼ë³´
            "biz.chosun.com",     # ì¡°ì„ ë¹„ì¦ˆ
            "asiae.co.kr",        # ì•„ì‹œì•„ê²½ì œ
            "edaily.co.kr",       # ì´ë°ì¼ë¦¬
            "news.heraldcorp.com",# í—¤ëŸ´ë“œê²½ì œ
            "fnnews.com",         # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
            "mt.co.kr",           # ë¨¸ë‹ˆíˆ¬ë°ì´
            "magazine.mk.co.kr",  # ë§¤ê²½ì´ì½”ë…¸ë¯¸
            "hankyung.com",       # í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤ (í•œêµ­ê²½ì œ ë™ì¼ ë„ë©”ì¸)
            "donga.com",          # ì£¼ê°„ë™ì•„ (ë™ì•„ì¼ë³´ ë™ì¼ ë„ë©”ì¸)
            "sisain.co.kr",       # ì‹œì‚¬IN
            "weekly.chosun.com"   # ì£¼ê°„ì¡°ì„ 
        ],
        queries = [
        #â–ª ì¡°ì§ êµ¬ì¡° ë° ì¸ì‚¬ ì œë„
        "ì¡°ì§ê°œí¸", "ì¸ì‚¬ì œë„"
        # , "ì§ë¬´ì¤‘ì‹¬", "ì§ë¬´ê¸‰ì œ", "ì„±ê³¼ê¸‰ì œ", "í˜¸ë´‰ì œ", "ì§ê¸‰ì²´ê³„", "ì—°ê³µì„œì—´"
    
        # #â–ª ì±„ìš© ë° ì¸ì¬ í™•ë³´
        # "ì±„ìš©ê³µê³ ", "ëŒ€ê¸°ì—… ê³µì±„", "ì‹ ì…ì±„ìš©", "ê²½ë ¥ì±„ìš©", "ì±„ìš©ì‹œì¥", "ë¸”ë¼ì¸ë“œì±„ìš©", "ê³ ìš©ì‹œì¥", "ê²½ë ¥ë‹¨ì ˆ", "ì±„ìš©ë©´ì ‘",
        # "ê³µì±„ íì§€", "ìƒì‹œì±„ìš©", "ì¸ì¬ í™•ë³´", "ë¦¬í¬ë£¨íŒ…", "ìˆ˜ì‹œì±„ìš©", "ê¸€ë¡œë²Œ ì¸ì¬","ì»¬ì²˜í•",
    
        # # â–ª ì¸ì‚¬í‰ê°€ ë° ë³´ìƒ
        # "ì¸ì‚¬í‰ê°€", "ì„±ê³¼í‰ê°€", "ì—­ëŸ‰í‰ê°€", "ë³´ìƒì²´ê³„", "ì—°ë´‰ì œ", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ",
        # "ì„ê¸ˆì¸ìƒ", "ê¸°ë³¸ê¸‰", "ì—°ì°¨ìˆ˜ë‹¹", "í†µìƒì„ê¸ˆ", "ì„ê¸ˆí”¼í¬", "ì„ê¸ˆì²´ê³„", "ìµœì €ì„ê¸ˆ", "ê³ ì •ê¸‰", "ì„±ê³¼ì—°ë´‰", "í‡´ì§ê¸ˆ",
        # "í‰ê· ì—°ë´‰", "ê·¼ì†", "í¬ê´„ì„ê¸ˆ", "ì‹¤ì—…ê¸‰ì—¬", "ì—°ê¸ˆê°œí˜", "ìŠ¤í†¡ì˜µì…˜", "RSU", "ì„ê¸ˆí˜‘ìƒ",
    
        # # ğŸ’¼ ì¸ì¬ ê°œë°œ (Talent Development)
        # # â–ª êµìœ¡ ë° ë¦¬ìŠ¤í‚¬ë§
        # "ì‚¬ë‚´ êµìœ¡", "ë¦¬ìŠ¤í‚¬ë§", "ì—…ìŠ¤í‚¬ë§", "êµìœ¡í›ˆë ¨", "HRD", "ì‚¬ë‚´ëŒ€í•™", "ì¡í¬ìŠ¤íŒ…", "ì‚¬ë‚´ ì–‘ì„±",
    
        # # â–ª ê²½ë ¥ ê°œë°œ ë° ìŠ¹ì§„
        # "ì»¤ë¦¬ì–´íŒ¨ìŠ¤", "ê²½ë ¥ê°œë°œ", "ì§ë¬´ìˆœí™˜", "ìŠ¹ì§„ì œë„", "ë¦¬ë”ì‹­ í”„ë¡œê·¸ë¨", "í›„ê³„ì ì–‘ì„±", "ì „ì§",
    
        # # ğŸ§‘â€ğŸ’» ê·¼ë¬´ í™˜ê²½ ë° ì¡°ì§ ë¬¸í™” (Workplace & Culture)
        # # â–ª ê·¼ë¬´ì œë„ ë° ìœ ì—°ê·¼ë¬´
        # "52ì‹œê°„", "ìœ ì—°ê·¼ë¡œ", "ì¬íƒê·¼ë¬´", "ìœ ì—°ê·¼ë¬´ì œ", "ì„ íƒê·¼ë¡œì œ", "ë‹¨ì¶• ê·¼ë¡œ", "ì£¼4ì¼ì œ", "ì£¼4.5ì¼ì œ", "ê·¼ë¬´ì‹œê°„", 
        # "ìœ¡ì•„íœ´ì§", "íœ´ì§ì œë„", "ì¶œì‚°íœ´ê°€","ê·¼ì†íœ´ê°€", "ì‚¬ë‚´ë³µì§€", "ë³µì§€ í¬ì¸íŠ¸",
    
        # # â–ª ì¡°ì§ë¬¸í™” ë° ë‹¤ì–‘ì„±
        # "ì¡°ì§ë¬¸í™”", "ì›Œë¼ë°¸", "DEI", "ì‚¬ë‚´ ì†Œí†µ",
    
        # # âš–ï¸ ì •ì±… ë° ë…¸ë™ë²• (HR Policy & Labor Law)
        # # â–ª ì •ë¶€ ì •ì±… ë° ì œë„ ë³€í™”
        # "ì¸ì‚¬ ì •ì±…", "ê³ ìš© ì •ì±…", "ë…¸ë™ì‹œì¥ ê°œí¸", "ì¸ì‚¬ í–‰ì •", "ì¸ì¬ ì •ì±…",
    
        # # â–ª ë…¸ë™ë²• ë° ê·œì œ
        # "ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ë²•", "ë…¸ë™ì‹œê°„", "ì¤‘ëŒ€ì¬í•´ë²•",
        # "ì§ì¥ ë‚´ ê´´ë¡­í˜", "ê³ ìš© ì•ˆì •ì„±", "ì‚°ì—…ì¬í•´", "ë…¸ì¡°í˜‘ì•½", "ì„ë‹¨í˜‘", "ë‹¨ì²´êµì„­",
        # "ê·¼ë¡œí™˜ê²½", "ê·¼ë¡œê³„ì•½", "ì •ë…„ì—°ì¥", "íŒŒì—…", "ì‹¤ì—…ì", "ë…¸ì¡°í™œë™", "ë…¸ì‚¬", "ë…¸ì‚¬ê°ˆë“±", "êµ¬ì¡°ì¡°ì •", "í¬ë§í‡´ì§",
    
        # # ğŸ“Š HR í…Œí¬ ë° ë°ì´í„° (HR Tech & Analytics)
        # # â–ª HR í…Œí¬ ë„ì… ì‚¬ë¡€
        # "HR Tech", "HR ìë™í™”", "ì¸ì‚¬ê´€ë¦¬ ì‹œìŠ¤í…œ", "HRIS",
    
        # # â–ª ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬
        # "ì¸ì‚¬ ë°ì´í„°", "HR Analytics", "ì¸ì‚¬ ë°ì´í„° ë¶„ì„", "ë°ì´í„° ê¸°ë°˜ í‰ê°€",
    
        # # ğŸ¢ ê·¸ë£¹/ê¸°ì—…ë³„ ì¸ì‚¬ í‚¤ì›Œë“œ (ëŒ€ê¸°ì—… ê´€ë ¨)
        # # â–ª ì‚¼ì„±
        # "ì‚¼ì„± ì¸ì‚¬", "ì‚¼ì„± ì„ì›", "ì‚¼ì„± ì¡°ì§", "ì‚¼ì„± ê²½ì˜ì§„", "ì‚¼ì„± ì¡°ì§ê°œí¸", "ì‚¼ì„± ì˜ì…", "ì‚¼ì„± ë‚´ì •", "ì‚¼ì„± ë°œë ¹", "ì‚¼ì„± ì‚¬ì™¸ì´ì‚¬",  "ì‚¼ì„± ì¶œì‹ ",
    
        # # â–ª í˜„ëŒ€í•´ìƒ
        # "í˜„ëŒ€í•´ìƒ ì¸ì‚¬", "í˜„ëŒ€í•´ìƒ ì„ì›", "í˜„ëŒ€í•´ìƒ ì¡°ì§", "í˜„ëŒ€í•´ìƒ ê²½ì˜ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§ê°œí¸", "í˜„ëŒ€í•´ìƒ ì˜ì…", "í˜„ëŒ€í•´ìƒ ë‚´ì •", "í˜„ëŒ€í•´ìƒ ë°œë ¹", "í˜„ëŒ€í•´ìƒ ì‚¬ì™¸ì´ì‚¬", "í˜„ëŒ€í•´ìƒ ì¶œì‹ ",
    
        # # â–ª DBì†ë³´
        # "DBì†ë³´ ì¸ì‚¬", "DBì†ë³´ ì„ì›", "DBì†ë³´ ì¡°ì§", "DBì†ë³´ ê²½ì˜ì§„", "DBì†ë³´ ì¡°ì§ê°œí¸", "DBì†ë³´ ì˜ì…", "DBì†ë³´ ë‚´ì •", "DBì†ë³´ ë°œë ¹", "DBì†ë³´ ì‚¬ì™¸ì´ì‚¬",  "DBì†ë³´ ì¶œì‹ ",
    
        # # â–ª KBì†ë³´
        # "KBì†ë³´ ì¸ì‚¬", "KBì†ë³´ ì„ì›", "KBì†ë³´ ì¡°ì§", "KBì†ë³´ ê²½ì˜ì§„", "KBì†ë³´ ì¡°ì§ê°œí¸", "KBì†ë³´ ì˜ì…", "KBì†ë³´ ë‚´ì •", "KBì†ë³´ ë°œë ¹", "KBì†ë³´ ì‚¬ì™¸ì´ì‚¬",  "KBì†ë³´ ì¶œì‹ ",
    
        # # â–ª ë©”ë¦¬ì¸ 
        # "ë©”ë¦¬ì¸  ì¸ì‚¬", "ë©”ë¦¬ì¸  ì„ì›", "ë©”ë¦¬ì¸  ì¡°ì§", "ë©”ë¦¬ì¸  ê²½ì˜ì§„", "ë©”ë¦¬ì¸  ì¡°ì§ê°œí¸", "ë©”ë¦¬ì¸  ì˜ì…", "ë©”ë¦¬ì¸  ë‚´ì •", "ë©”ë¦¬ì¸  ë°œë ¹", "ë©”ë¦¬ì¸  ì‚¬ì™¸ì´ì‚¬"  "ë©”ë¦¬ì¸  ì¶œì‹ "
    
       ])

    # CSVë¡œ ì €ì¥
    output_file = "crawled_news.csv"
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"\nğŸ“„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
