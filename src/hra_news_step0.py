# src/hra_news_step0.py

import urllib.request
import urllib.parse
import json
import pandas as pd
from datetime import datetime, timedelta
import html
import os
import sys

from utils.logger import log_info, log_error
from utils.file_manager import get_today_folder, get_today_filename


def search_naver_news_multi(queries, client_id, client_secret, display=300, filter_press_names=[]):
    all_results = []
    start = (datetime.now() - timedelta(days=1)).date()

    for query in queries:
        log_info(f"ğŸ” ê²€ìƒ‰ì–´ '{query}' ì²˜ë¦¬ ì¤‘...")
        query_results = []

        for start_index in range(1, min(display, 1000)+1, 100):
            encText = urllib.parse.quote(query)
            url = f"https://openapi.naver.com/v1/search/news?query={encText}&display=100&start={start_index}&sort=date"

            request = urllib.request.Request(url)
            request.add_header("X-Naver-Client-Id", client_id)
            request.add_header("X-Naver-Client-Secret", client_secret)

            try:
                response = urllib.request.urlopen(request)
            except Exception as e:
                log_error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e} (query: '{query}')")
                continue

            if response.getcode() != 200:
                log_error(f"âŒ Error Code: {response.getcode()} for query '{query}'")
                continue

            response_body = response.read().decode('utf-8')
            news_data = json.loads(response_body)

            for item in news_data['items']:
                title = html.unescape(item['title'].replace('<b>', '').replace('</b>', ''))
                description = html.unescape(item['description'].replace('<b>', '').replace('</b>', ''))
                link = item['link']
                pubDate = datetime.strptime(item['pubDate'], "%a, %d %b %Y %H:%M:%S %z")

                if 'originallink' in item:
                    parts = item['originallink'].split('/')
                else:
                    parts = item['link'].split('/')
                press_name = parts[2] if len(parts) > 2 else ''

                if pubDate.date() < start:
                    continue

                if filter_press_names and not any(name in press_name for name in filter_press_names):
                    continue

                query_results.append({
                    "ê²€ìƒ‰ì–´": query,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": description,
                    "ë‚ ì§œ": pubDate.strftime("%Y-%m-%d"),
                    "ì–¸ë¡ ì‚¬": press_name
                })

        log_info(f"âœ… '{query}' ì™„ë£Œ - {len(query_results)}ê±´ ìˆ˜ì§‘")
        all_results.extend(query_results)

       # ê²°ê³¼ DF êµ¬ì„±
        df = pd.DataFrame(all_results)
    
        # ë¹ˆ ê²°ê³¼ë©´ ì»¬ëŸ¼ë§Œ ë§ì¶°ì„œ ë°˜í™˜ (step1ì—ì„œ ì»¬ëŸ¼ ê¸°ëŒ€ ì¶©ì¡±)
        if df.empty:
            return pd.DataFrame(columns=["êµ¬ë¶„","í‚¤ì›Œë“œ","ì¼ì","í—¤ë“œë¼ì¸","ìš”ì•½","ë§¤ì²´ëª…","URL","row_id"])
    
        # ì¤‘ë³µ/í•„í„°/ì¬êµ¬ì„±
        df = df.drop_duplicates(subset=["URL"])
        df = df[df["URL"].astype(str).startswith("https://n.news.naver.com/mnews")].reset_index(drop=True)
    
        df = pd.DataFrame({
            "êµ¬ë¶„": "",
            "í‚¤ì›Œë“œ": df["ê²€ìƒ‰ì–´"],
            "ì¼ì": df["ë‚ ì§œ"],
            "í—¤ë“œë¼ì¸": df["ì œëª©"],
            "ìš”ì•½": df["ìš”ì•½"],
            "ë§¤ì²´ëª…": df["ì–¸ë¡ ì‚¬"],
            "URL": df["URL"]
        }).reset_index(drop=True)
    
        # step1 ì•ˆì „ì„± ìœ„í•´ ì ˆëŒ€ ìœ„ì¹˜ id ì‹¬ê¸°
        df["row_id"] = df.index  # 0..n-1
    
        # â›”ï¸ ì—¬ê¸° ìˆë˜ df.to_csv(output_file, ...) ë¼ì¸ì€ ì‚­ì œ!
  return df




def main():
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        log_error("âŒ NAVER_CLIENT_ID, NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    queries = [
    
    # ğŸ¢ ì¸ì‚¬ ì „ëµ (HR Strategy)
    # â–ª ì¡°ì§ êµ¬ì¡° ë° ì¸ì‚¬ ì œë„
    "ì¡°ì§ê°œí¸", "ì¸ì‚¬ì œë„", "ì§ë¬´ì¤‘ì‹¬ ì¸ì‚¬", "ì§ë¬´ê¸‰ì œ", "ì„±ê³¼ê¸‰ì œ", "í˜¸ë´‰ì œ", "ì§ê¸‰ì²´ê³„", "CVC",

    # â–ª ì±„ìš© ë° ì¸ì¬ í™•ë³´
    "ì±„ìš©ê³µê³ ", "ì‹ ì…ê³µì±„", "ì»¬ì²˜í•", "ì»¬ì³í•", "ì‹ ì…ì±„ìš©", "ê²½ë ¥ì±„ìš©",
    "ì±„ìš©ì‹œì¥", "ë¸”ë¼ì¸ë“œì±„ìš©", "ì±„ìš©ê³µì •ì„±", "ê³ ìš©ì‹œì¥", "ê²½ë ¥ë‹¨ì ˆ", "ì±„ìš©ë©´ì ‘",
    "ê³µì±„ íì§€", "ìƒì‹œì±„ìš©", "ì¸ì¬ í™•ë³´", "ë¦¬í¬ë£¨íŒ…", "ì±„ìš© ì „ëµ", "ìˆ˜ì‹œì±„ìš©", "ê¸€ë¡œë²Œ ì¸ì¬", "ì¸ì¬ìˆ˜í˜ˆ", "ì„œì¹˜íŒ",

    # â–ª ì¸ì‚¬í‰ê°€ ë° ë³´ìƒ
    "ì¸ì‚¬í‰ê°€", "ì„±ê³¼í‰ê°€", "ì—­ëŸ‰í‰ê°€", "ë³´ìƒì²´ê³„", "ì—°ë´‰ì œ", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ",
    "ì„ê¸ˆì¸ìƒ", "ì„ê¸ˆí˜‘ìƒ", "ê¸°ë³¸ê¸‰", "ì—°ì°¨ìˆ˜ë‹¹", "í†µìƒì„ê¸ˆ", "ì„ê¸ˆí”¼í¬",
    "ì„ê¸ˆì²´ê³„", "ì‹œê¸‰ì œ", "ìµœì €ì„ê¸ˆ", "ê³ ì •ê¸‰", "ì„±ê³¼ì—°ë´‰", "í‡´ì§ê¸ˆ", "ì„±ê³¼ë³´ìˆ˜ì²´ê³„",
    "í‰ê· ì—°ë´‰", "ê·¼ì†", "í¬ê´„ì„ê¸ˆ", "ì‹¤ì—…ê¸‰ì—¬", "ì—°ê¸ˆê°œí˜","ìŠ¤í†¡ì˜µì…˜","RSU","ì„ê¸ˆë¶„í¬ì œ","í‡´ì§ê¸ˆëˆ„ì§„ì œ","í‰ê·  ê¸‰ì—¬", "ì—°ë´‰í‚¹",

    # ğŸ’¼ ì¸ì¬ ê°œë°œ (Talent Development)
    # â–ª êµìœ¡ ë° ë¦¬ìŠ¤í‚¬ë§
    "ì‚¬ë‚´ êµìœ¡", "ë¦¬ìŠ¤í‚¬ë§", "ì—…ìŠ¤í‚¬ë§", "êµìœ¡í›ˆë ¨", "HRD", "ì‚¬ë‚´ëŒ€í•™","ì¡í¬ìŠ¤íŒ…", "ì‚¬ë‚´ ì–‘ì„±", "ì¸ì  ìë³¸",

    # â–ª ê²½ë ¥ ê°œë°œ ë° ìŠ¹ì§„
    "ì»¤ë¦¬ì–´íŒ¨ìŠ¤", "ê²½ë ¥ê°œë°œ", "ì§ë¬´ìˆœí™˜", "ìŠ¹ì§„ì œë„", "ë¦¬ë”ì‹­ í”„ë¡œê·¸ë¨", "í›„ê³„ì ì–‘ì„±", "ì „ì§","ì¡°ê¸° í‡´ì‚¬", "ì¡°ìš©í•œ í‡´ì‚¬", "ë² ì´ë¹„ë¶ í‡´ì§",

    # ğŸ§‘â€ğŸ’» ê·¼ë¬´ í™˜ê²½ ë° ì¡°ì§ ë¬¸í™” (Workplace & Culture)
    # â–ª ê·¼ë¬´ì œë„ ë° ìœ ì—°ê·¼ë¬´
    "52ì‹œê°„", "ìœ ì—°ê·¼ë¡œ", "ì¬íƒê·¼ë¬´", "ìœ ì—°ê·¼ë¬´ì œ", "ì§ì¥ ë‚´ ì–´ë¦°ì´ì§‘",
    "ì„ íƒê·¼ë¡œì œ", "ê·¼ë¡œì‹œê°„ ë‹¨ì¶•", "ì£¼4ì¼ì œ", "4.5ì¼ì œ", "ë‹¨ì¶•ê·¼ë¡œ", "ê·¼ë¬´ì‹œê°„", "ìœ¡ì•„íœ´ì§", 
    "íœ´ì§ì œë„", "ì¶œì‚°íœ´ê°€", "ê·¼ì†íœ´ê°€", "ì‚¬ë‚´ë³µì§€","ê·¼ë¬´ê¸°ê°•", "ì§ì› ê¸°ê°•", "ì„ì§ì› êµ°ê¸°","ë¹„ìƒ ê²½ì˜", "ëŒ€ê¸°ì—… ì‚¬ì—… ì¶•ì†Œ", "ëŒ€ê¸°ì—… íˆ¬ì ì¶•ì†Œ ì² íšŒ",
    "ì£¼ 6ì¼ ì¶œê·¼", "ì‚¬ì¥ë‹¨ íšŒì˜","ì¡°ì§ê³ ë ¹í™”","ê³ ìš©ì¬ì„¤ê³„","ëŒ€ê¸°ì—… ì„¸ëŒ€ ì—­ì „", "ê¸°ì—… ì¸ë ¥ êµ¬ì¡°","ì—°ì°¨ ì‚¬ìš©", "íœ´ê°€ ì¼ìˆ˜", "ì—°ì°¨ ìˆ˜ë‹¹",

    # â–ª ì¡°ì§ë¬¸í™” ë° ë‹¤ì–‘ì„±
    "ì¡°ì§ë¬¸í™”", "ì›Œë¼ë°¸", "DEI", "ì‚¬ë‚´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ì¡°ì§ë¬¸í™” í˜ì‹ ", "ì»¬ì³ë±", "ì»¬ì³ ì—ë°˜ì ¤ë¦¬ìŠ¤íŠ¸",

    # âš–ï¸ ì •ì±… ë° ë…¸ë™ë²• (HR Policy & Labor Law)
    # â–ª ì •ë¶€ ì •ì±… ë° ì œë„ ë³€í™”
    "ì¸ì‚¬ ì •ì±…", "ê³ ìš© ì •ì±…", "ë…¸ë™ì‹œì¥ ê°œí¸", "ì¸ì‚¬ í–‰ì •", "ì¸ì¬ ì •ì±…","ë²•ì¸ì„¸", "ìƒë²• ê°œì •", "ë¦¬ì‡¼ì–´ë§","ë°°ì„ì£„","íš¡ë ¹ì£„","íš¡ì¬ì„¸","êµìœ¡ì„¸","ìƒìƒ ê¸ˆìœµ",

    # â–ª ë…¸ë™ë²• ë° ê·œì œ
    "ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ë²•", "ë…¸ë™ì‹œê°„", "ì¤‘ëŒ€ì¬í•´ë²•","ë…¸ë€ë´‰íˆ¬ë²•","ì±…ë¬´êµ¬ì¡°ë„","ì§€ë°°êµ¬ì¡°","ë¡œíŒ ë…¸ë™",
    "ì§ì¥ ë‚´ ê´´ë¡­í˜", "ê³ ìš© ì•ˆì •ì„±", "ì‚°ì—…ì¬í•´", "ë…¸ì¡°í˜‘ì•½", "ì„ë‹¨í˜‘", "ë‹¨ì²´êµì„­", "ë‹¨ì²´í˜‘ì•½",
    "ê·¼ë¡œí™˜ê²½", "ê·¼ë¡œê³„ì•½", "ì •ë…„ì—°ì¥", "íŒŒì—…", "ì‹¤ì—…ì", "ë…¸ì¡°í™œë™", "ì¬ê³ ìš©", "ë…¸ë™ì¡°í•©ë²•","ë‹¨ì²´êµì„­ê¶Œ",
    "ë…¸ì‚¬", "ë…¸ì‚¬ê°ˆë“±", "êµ¬ì¡°ì¡°ì •", "ì„ê¸ˆí˜‘ìƒ", "í¬ë§í‡´ì§", "ì¸ì›ê°ì¶•", "ì‚¬ì—… ê°œí¸", "í”„ë¼ì„ ì˜¤í”¼ìŠ¤","ì¤‘ëŒ€ì¬í•´ì‚¬ê³ ","ë…¸ë™ ì •ì±…",

    # ğŸ“Š HR í…Œí¬ ë° ë°ì´í„° (HR Tech & Analytics)
    # â–ª HR í…Œí¬ ë„ì… ì‚¬ë¡€
    "HR Tech", "HR ìë™í™”", "ì¸ì‚¬ê´€ë¦¬ ì‹œìŠ¤í…œ", "HRIS","ì •ë³´ë³´í˜¸","CISO",

    # â–ª ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬
    "ì¸ì‚¬ ë°ì´í„°", "HR Analytics", "People Analytics","ë°ì´í„° ê¸°ë°˜ í‰ê°€","AI ê¸°ë°˜ ì±„ìš©", "AI ì±„ìš© ë„ì…", "ì±„ìš© AI í™œìš©", "AI ë©´ì ‘" ,

    # ğŸ¢ ê·¸ë£¹/ê¸°ì—…ë³„ ì¸ì‚¬ í‚¤ì›Œë“œ (ëŒ€ê¸°ì—… ê´€ë ¨)
    "ì‚¼ì„± ì¸ì‚¬", "ì‚¼ì„± ì„ì›ì§„", "ì‚¼ì„± ì¡°ì§", "ì‚¼ì„± ê²½ì˜ì§„", "ì‚¼ì„± ì¡°ì§ê°œí¸", "ì‚¼ì„± ì˜ì…", "ì‚¼ì„± ì¶œì‹ ", "ì‚¼ì„± ë°œë ¹", "ì‚¼ì„± ì‚¬ì™¸ì´ì‚¬","ì‚¼ì„± M&A","ì‚¼ì„± ì‚¬ì—…ì§€ì› T/F","ì‚¼ì„± ë¦¬ë”ì‹­",
    "í˜„ëŒ€í•´ìƒ ì¸ì‚¬", "í˜„ëŒ€í•´ìƒ ì„ì›ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§", "í˜„ëŒ€í•´ìƒ ê²½ì˜ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§ê°œí¸", "í˜„ëŒ€í•´ìƒ ì˜ì…", "í˜„ëŒ€í•´ìƒ ì¶œì‹ ", "í˜„ëŒ€í•´ìƒ ì‚¬ì™¸ì´ì‚¬",
    "DBì†ë³´ ì¸ì‚¬", "DBì†ë³´ ì„ì›ì§„", "DBì†ë³´ ì¡°ì§", "DBì†ë³´ ê²½ì˜ì§„", "DBì†ë³´ ì¡°ì§ê°œí¸", "DBì†ë³´ ì˜ì…", "DBì†ë³´ ì¶œì‹ ", "DBì†ë³´ ì‚¬ì™¸ì´ì‚¬",
    "KBì†ë³´ ì¸ì‚¬", "KBì†ë³´ ì„ì›ì§„", "KBì†ë³´ ì¡°ì§", "KBì†ë³´ ê²½ì˜ì§„", "KBì†ë³´ ì¡°ì§ê°œí¸", "KBì†ë³´ ì˜ì…", "KBì†ë³´ ì¶œì‹ ", "KBì†ë³´ ì‚¬ì™¸ì´ì‚¬",
    "ë©”ë¦¬ì¸  ì¸ì‚¬", "ë©”ë¦¬ì¸  ì„ì›ì§„", "ë©”ë¦¬ì¸  ì¡°ì§", "ë©”ë¦¬ì¸  ê²½ì˜ì§„", "ë©”ë¦¬ì¸  ì¡°ì§ê°œí¸", "ë©”ë¦¬ì¸  ì˜ì…", "ë©”ë¦¬ì¸  ì¶œì‹ ", "ë©”ë¦¬ì¸  ì‚¬ì™¸ì´ì‚¬","GA ì˜ì…", "GA ì¡°ì§ê°œí¸",
    "ì†ë³´ì‚¬ ì¡°ì§ ê°œí¸", "ìƒë³´ì‚¬ ì¡°ì§ ê°œí¸", "ë³´í—˜ì‚¬ ì‚¬ì™¸ì´ì‚¬", "ì†ë³´ì‚¬ ìƒë³´ì‚¬ ì‚¬ì™¸ì´ì‚¬", "ì†ë³´ ì „ëµì  ì œíœ´", "ìƒë³´ ì „ëµì  ì œíœ´",
    "ì†ë³´ ë…¸ì¡°", "ìƒë³´ ë…¸ì¡°", "ì¹´ë“œ ë…¸ì¡°", "ì€í–‰ ë…¸ì¡°", "ì¦ê¶Œ ë…¸ì¡°", "ê¸ˆìœµê¶Œ ì¡°ì§", "ë³´í—˜ì‚¬ ì¡°ì§", "ì†ë³´ ì˜ì…", "ê¸ˆìœµê¶Œ ì˜ì…", "ë³´í—˜ì—…ê³„ ì˜ì…", "ë³´í—˜ì—…ê³„ ì‹ ì„¤", "ì¸ì‚¬ ë‹´ë‹¹ ì„ì›", "HR ë‹´ë‹¹ ì„ì›",
    "ê·¸ë£¹ ì´ìˆ˜","ì€í–‰ ì—°í•©íšŒ", "ì—¬ì‹  ê¸ˆìœµ í˜‘íšŒ", "4ëŒ€ ê·¸ë£¹", "ê¸ˆìœµ ê·¸ë£¹", "ê¸ˆìœµ ì§€ì£¼"]

    df = search_naver_news_multi(
        queries=queries,
        client_id=client_id,
        client_secret=client_secret,
        display=300,
        filter_press_names=[
            "www.chosun.com", "joins.com", "donga.com", "khan.co.kr", "hani.co.kr",
            "hankyung.com", "mk.co.kr", "hankookilbo.com", "biz.chosun.com",
            "asiae.co.kr", "edaily.co.kr", "news.heraldcorp.com", "fnnews.com",
            "mt.co.kr", "magazine.mk.co.kr", "sisain.co.kr", "weekly.chosun.com", "insnews.co.kr", "insjournal.co.kr", "insweek.co.kr"
        ]
    )

    output_dir = get_today_folder()
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, get_today_filename("step0_raw.csv"))

    df.to_csv(output_file, index=False, encoding="utf-8-sig", errors='ignore')
    log_info(f"ğŸ“„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")


if __name__ == "__main__":
    main()
