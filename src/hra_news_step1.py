# src/hra_news_step1.py - GPT ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ì„ ë³„ (ë³¸ë¬¸ ìˆ˜ì§‘ ì—†ìŒ)

import os
import sys
import pandas as pd
import html
from datetime import datetime
from utils.logger import log_info, log_error
from utils.file_manager import get_today_folder, get_today_filename
from utils.gpt_utils import analyze_articles_batch
from utils.gpt_utils import deduplicate_news_with_gpt
from utils.gpt_utils import deduplicate_news_with_gpt_twopass 

def main():
    log_info("ğŸ“„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    today_folder = get_today_folder()
    input_file = os.path.join(today_folder, get_today_filename("step0_raw.csv"))
    output_file = os.path.join(today_folder, get_today_filename("step1_filtered.csv"))

    try:
        df = pd.read_csv(input_file, encoding="utf-8-sig")
    except Exception as e:
        log_error(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        sys.exit(1)

    log_info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")

    # âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í•œê¸€ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
    domain_to_korean = {
        "shindonga.donga.com": "ì›”ê°„ ì‹ ë™ì•„",
        "magazine.hankyung.com": "ë§¤ê±°ì§„ í•œê²½",
        "www.chosun.com": "ì¡°ì„ ì¼ë³´",
        "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ",
        "weekly.chosun.com": "ì£¼ê°„ì¡°ì„ ",
        "www.joins.com": "ì¤‘ì•™ì¼ë³´",
        "www.donga.com": "ë™ì•„ì¼ë³´",
        "weekly.donga.com": "ì£¼ê°„ë™ì•„",
        "www.khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
        "weekly.khan.co.kr": "ì£¼ê°„ê²½í–¥",
        "www.hani.co.kr": "í•œê²¨ë ˆ",
        "www.hankyung.com": "í•œêµ­ê²½ì œ",
        "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "www.magazine.mk.co.kr": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
        "www.hankookilbo.com": "í•œêµ­ì¼ë³´",
        "view.asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
        "www.edaily.co.kr": "ì´ë°ì¼ë¦¬",
        "news.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
        "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "news.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
        "www.sisain.co.kr": "ì‹œì‚¬IN",
        "sports.khan.co.kr" : "ìŠ¤í¬ì¸ ê²½í–¥",
        "sports.donga.com" : "ìŠ¤í¬ì¸ ë™ì•„",
        "insweek.co.kr" : "ë³´í—˜ì‹ ë³´",
        "insjournal.co.kr" : "ë³´í—˜ì €ë„",
        "insnews.co.kr" : "í•œêµ­ë³´í—˜ì‹ ë¬¸"
    }
    df["ë§¤ì²´ëª…"] = df["ë§¤ì²´ëª…"].map(domain_to_korean).fillna(df["ë§¤ì²´ëª…"])

    # âœ… ì œëª© ì •ë¦¬: HTML ì œê±° + ëŒ€ê´„í˜¸ ì œê±°
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].apply(html.unescape)
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].str.replace(r"\[.*?\]", "", regex=True).str.strip()

    
    # âœ… ì¤‘ë³µ ì œê±° ë¨¼ì €
    df = deduplicate_news_with_gpt_twopass(df)
    
    # âœ… GPT ë¶„ì„ ì‹¤í–‰
    df = analyze_articles_batch(df)

    # âœ… ì¤‘ìš”ë„ 3 ì´ìƒë§Œ í•„í„°ë§
    df = df[df["ì¤‘ìš”ë„"] >= 3].reset_index(drop=True)
    log_info(f"âœ¨ ì¤‘ìš”ë„ 3 ì´ìƒ ê¸°ì‚¬ ìˆ˜: {len(df)}ê±´")

    # âœ… ì €ì¥
    os.makedirs(today_folder, exist_ok=True)
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    log_info(f"âœ… ì¤‘ìš” ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: {output_file}")


if __name__ == "__main__":
    main()
