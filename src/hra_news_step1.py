# src/hra_news_step1.py - GPT ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ì„ ë³„ (ë³¸ë¬¸ ìˆ˜ì§‘ ì—†ìŒ)

import os
import sys
import pandas as pd
import html
from datetime import datetime
from utils.logger import log_info, log_error
from utils.file_manager import get_today_folder, get_today_filename
from utils.gpt_utils import analyze_articles_batch
from utils.gpt_utils import deduplicate_news_with_gpt
from utils.gpt_utils import deduplicate_news_with_gpt_twopass

def _normalize_headline(s: pd.Series) -> pd.Series:
    # ëŒ€ê´„í˜¸/ê³µë°±/ì¤‘ë³µê³µë°± ë“± ì •ê·œí™” (í´ë°± ì¤‘ë³µì œê±°ìš©)
    return (
        s.fillna("")
         .str.replace(r"\[.*?\]", "", regex=True)
         .str.replace(r"\s+", " ", regex=True)
         .str.strip()
         .str.lower()
    )

def _safe_twopass_dedupe(df: pd.DataFrame) -> pd.DataFrame:
    """
    GPT 2íŒ¨ìŠ¤ ì¤‘ë³µì œê±°ë¥¼ ì•ˆì „í•˜ê²Œ ê°ì‹¸ëŠ” ë˜í¼.
    - 1ì°¨: reset_index í›„ 2íŒ¨ìŠ¤ ì‹œë„
    - 2ì°¨: ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ í•œ ë²ˆ ê°•ì œ reset í›„ ì¬ì‹œë„
    - 3ì°¨(í´ë°±): í—¤ë“œë¼ì¸/URL ê¸°ë°˜ì˜ ì „í†µì  ì¤‘ë³µì œê±°
    """
    base = df.reset_index(drop=True).copy()
    try:
        return deduplicate_news_with_gpt_twopass(base.reset_index(drop=True))
    except IndexError as e:
        log_error(f"[WARN] GPT 2íŒ¨ìŠ¤ ì¤‘ë³µì œê±° IndexError 1ì°¨ ë°œìƒ: {e}. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            return deduplicate_news_with_gpt_twopass(base.reset_index(drop=True))
        except Exception as e2:
            log_error(f"[FALLBACK] GPT 2íŒ¨ìŠ¤ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}. ê·œì¹™ ê¸°ë°˜ ì¤‘ë³µì œê±°ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            fb = base.copy()
            # ê·œì¹™ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: URL ìš°ì„ , ê·¸ ë‹¤ìŒ ì •ê·œí™” í—¤ë“œë¼ì¸
            fb["__norm_headline__"] = _normalize_headline(fb["í—¤ë“œë¼ì¸"])
            fb = fb.drop_duplicates(subset=["URL"])
            fb = fb.drop_duplicates(subset=["__norm_headline__"])
            fb = fb.drop(columns=["__norm_headline__"])
            fb = fb.reset_index(drop=True)
            return fb

def main():
    log_info("ğŸ“„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    today_folder = get_today_folder()
    input_file = os.path.join(today_folder, get_today_filename("step0_raw.csv"))
    output_file = os.path.join(today_folder, get_today_filename("step1_filtered.csv"))

    try:
        df = pd.read_csv(input_file, encoding="utf-8-sig")
    except Exception as e:
        log_error(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # âš™ï¸ ì…ë ¥ ì•ˆì •í™”: URL ê²°ì¸¡ ì œê±°, ì—°ì† ì¸ë±ìŠ¤ + row_id ë¶€ì—¬
    if "URL" not in df.columns:
        log_error("âŒ ì…ë ¥ ë°ì´í„°ì— URL ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    df = df[df["URL"].notna()].reset_index(drop=True).copy()
    if "row_id" not in df.columns:
        df["row_id"] = df.index  # 0..n-1 ê³ ì • ID

    log_info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")

    # âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í•œê¸€ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
    domain_to_korean = {
        "shindonga.donga.com": "ì›”ê°„ ì‹ ë™ì•„",
        "magazine.hankyung.com": "ë§¤ê±°ì§„ í•œê²½",
        "www.chosun.com": "ì¡°ì„ ì¼ë³´",
        "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ",
        "weekly.chosun.com": "ì£¼ê°„ì¡°ì„ ",
        "www.joins.com": "ì¤‘ì•™ì¼ë³´",
        "www.donga.com": "ë™ì•„ì¼ë³´",
        "weekly.donga.com": "ì£¼ê°„ë™ì•„",
        "www.khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
        "weekly.khan.co.kr": "ì£¼ê°„ê²½í–¥",
        "www.hani.co.kr": "í•œê²¨ë ˆ",
        "www.hankyung.com": "í•œêµ­ê²½ì œ",
        "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "www.magazine.mk.co.kr": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
        "www.hankookilbo.com": "í•œêµ­ì¼ë³´",
        "view.asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
        "www.edaily.co.kr": "ì´ë°ì¼ë¦¬",
        "news.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
        "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "news.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
        "www.sisain.co.kr": "ì‹œì‚¬IN",
        "sports.khan.co.kr": "ìŠ¤í¬ì¸ ê²½í–¥",
        "sports.donga.com": "ìŠ¤í¬ì¸ ë™ì•„",
        "insweek.co.kr": "ë³´í—˜ì‹ ë³´",
        "insjournal.co.kr": "ë³´í—˜ì €ë„",
        "insnews.co.kr": "í•œêµ­ë³´í—˜ì‹ ë¬¸"
    }
    df["ë§¤ì²´ëª…"] = df["ë§¤ì²´ëª…"].map(domain_to_korean).fillna(df["ë§¤ì²´ëª…"])

    # âœ… ì œëª© ì •ë¦¬: HTML ì œê±° + ëŒ€ê´„í˜¸ ì œê±°
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].apply(html.unescape)
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].str.replace(r"\[.*?\]", "", regex=True).str.strip()

    # ====== â›³ï¸ ì¤‘ìš”: GPT ì¤‘ë³µ ì œê±°(ì•ˆì „ ë˜í¼ ì‚¬ìš©) ======
    # ë‚´ë¶€ì—ì„œ ë°°ì¹˜ ìŠ¬ë¼ì´ì‹± ì‹œ iloc ë²”ìœ„ ë¬¸ì œê°€ ìƒê¸°ì§€ ì•Šë„ë¡ ì…ë ¥ì„ í•­ìƒ ì—°ì† ì¸ë±ìŠ¤ë¡œ ì œê³µ
    df = df.reset_index(drop=True)
    df = _safe_twopass_dedupe(df)
    df = df.reset_index(drop=True)

    # ====== GPT ë¶„ì„ ì‹¤í–‰ ì „ì—ë„ ì¸ë±ìŠ¤ ì •ë¦¬ ======
    df = df.reset_index(drop=True)
    df = analyze_articles_batch(df)

    # âœ… ì¤‘ìš”ë„ 3 ì´ìƒë§Œ í•„í„°ë§
    if "ì¤‘ìš”ë„" not in df.columns:
        log_error("âŒ GPT ë¶„ì„ ê²°ê³¼ì— 'ì¤‘ìš”ë„' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    df = df[df["ì¤‘ìš”ë„"] >= 3].reset_index(drop=True)
    log_info(f"âœ¨ ì¤‘ìš”ë„ 3 ì´ìƒ ê¸°ì‚¬ ìˆ˜: {len(df)}ê±´")

    # âœ… ì €ì¥
    os.makedirs(today_folder, exist_ok=True)
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    log_info(f"âœ… ì¤‘ìš” ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: {output_file}")

if __name__ == "__main__":
    main()
