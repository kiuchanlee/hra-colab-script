# src/hra_news_step1.py - GPT ê¸°ë°˜ ì¤‘ìš” ê¸°ì‚¬ ì„ ë³„ (ë³¸ë¬¸ ìˆ˜ì§‘ ì—†ìŒ)


import os
import sys
import pandas as pd
import html
from utils.logger import log_info, log_error
from utils.file_manager import get_today_folder, get_today_filename
from utils.gpt_utils import analyze_articles_batch, deduplicate_news_with_gpt_twopass

def _normalize_headline(s: pd.Series) -> pd.Series:
    """ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì œëª© ì •ê·œí™”"""
    return (
        s.fillna("")
         .str.replace(r"\[.*?\]", "", regex=True)
         .str.replace(r"\s+", " ", regex=True)
         .str.strip()
         .str.lower()
    )

def _safe_twopass_dedupe(df: pd.DataFrame) -> pd.DataFrame:
    """GPT ì¤‘ë³µì œê±° ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì—†ì´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜"""
    if df.empty: return df
    base = df.reset_index(drop=True).copy()
    try:
        return deduplicate_news_with_gpt_twopass(base)
    except Exception as e:
        log_error(f"âš ï¸ GPT ì¤‘ë³µì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ(ê±´ë„ˆëœ€): {e}")
        # í´ë°±: URL ë° ì œëª© ê¸°ë°˜ ë‹¨ìˆœ ì¤‘ë³µ ì œê±°
        fb = base.copy()
        fb["__norm__"] = _normalize_headline(fb["í—¤ë“œë¼ì¸"])
        fb = fb.drop_duplicates(subset=["URL"]).drop_duplicates(subset=["__norm__"])
        return fb.drop(columns=["__norm__"]).reset_index(drop=True)

def main():
    log_info("ğŸ“„ Step 1: ì¤‘ìš” ê¸°ì‚¬ ì„ ë³„ ë° í•„í„°ë§ ì‹œì‘")
    today_folder = get_today_folder()
    input_file = os.path.join(today_folder, get_today_filename("step0_raw.csv"))
    output_file = os.path.join(today_folder, get_today_filename("step1_filtered.csv"))

    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ë°ì´í„° ë¡œë“œ ì²´í¬
    if not os.path.exists(input_file):
        log_error(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        sys.exit(1)

    try:
        df = pd.read_csv(input_file, encoding="utf-8-sig")
    except Exception as e:
        log_error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 2. ë°ì´í„°ê°€ 0ê±´ì´ê±°ë‚˜ URL ì»¬ëŸ¼ì´ ì—†ì„ ë•Œì˜ ë°©ì–´ (KeyError ë°©ì§€ í•µì‹¬)
    if df.empty or "URL" not in df.columns:
        log_info("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ 0ê±´ì…ë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        # Step 2ì—ì„œ ê¸°ëŒ€í•˜ëŠ” ìµœì†Œí•œì˜ ì»¬ëŸ¼ êµ¬ì¡° ìƒì„±
        empty_df = pd.DataFrame(columns=["êµ¬ë¶„", "í‚¤ì›Œë“œ", "ì¼ì", "í—¤ë“œë¼ì¸", "ìš”ì•½", "ë§¤ì²´ëª…", "URL", "row_id", "ì¤‘ìš”ë„"])
        empty_df.to_csv(output_file, index=False, encoding="utf-8-sig")
        return

    # 3. ì–¸ë¡ ì‚¬ ë§¤í•‘ (ë„ë©”ì¸ ê¸°ë°˜)
    domain_to_korean = {
        "www.chosun.com": "ì¡°ì„ ì¼ë³´", "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ", 
        "www.hankyung.com": "í•œêµ­ê²½ì œ", "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "www.edaily.co.kr": "ì´ë°ì¼ë¦¬", "www.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
        "www.insnews.co.kr": "í•œêµ­ë³´í—˜ì‹ ë¬¸", "www.insjournal.co.kr": "ë³´í—˜ì €ë„",
        "www.joins.com" : "ì¤‘ì•™ì¼ë³´", "www.donga.com" : "ë™ì•„ì¼ë³´",
        "magazine.hankyung.com" : "ë§¤ê±°ì§„í•œê²½", "www.hani.co.kr" : "í•œê²¨ë ˆ",
        "www.hankookilbo.com" : "í•œêµ­ì¼ë³´", "view.asiae.co.kr" : "ì•„ì‹œì•„ê²½ì œ",
        "heraldcorp.com" : "í—¤ëŸ´ë“œê²½ì œ", "www.fnnews.com" : "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "mk.co.kr" : "ë§¤ì¼ê²½ì œ", "www.sisain.co.kr" : "ì‹œì‚¬ì¸", 
        "weekly.chosun.com" : "ì£¼ê°„ì¡°ì„ ", "insweek.co.kr" : "ë³´í—˜ì‹ ë³´",
        "www.khan.co.kr" : "ê²½í–¥ì‹ ë¬¸", "weekly.khan.co.kr" : "ì£¼ê°„ê²½í–¥", 
        "weekly.donga.com" : "ì£¼ê°„ë™ì•„",  "mbn.mk.co.kr" : "ë§¤ì¼ê²½ì œ",
         "sports.donga.com" : "ìŠ¤í¬ì¸ ë™ì•„", "sports.khan.co.kr" : "ê²½í–¥ìŠ¤í¬ì¸ "
    }
    df["ë§¤ì²´ëª…"] = df["ë§¤ì²´ëª…"].map(domain_to_korean).fillna(df["ë§¤ì²´ëª…"])

    # 4. í…ìŠ¤íŠ¸ ì •ì œ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].apply(html.unescape)
    df = df.reset_index(drop=True)

    # 5. GPT ì¤‘ë³µ ì œê±° (ì•ˆì „ ë˜í¼)
    df = _safe_twopass_dedupe(df)
    
    # 6. GPT ê¸°ì‚¬ ë¶„ì„ (ì¤‘ìš”ë„ íŒë³„)
    df = df.reset_index(drop=True)
    df["row_id"] = df.index # GPT ì‘ë‹µê³¼ ë§¤ì¹­ì„ ìœ„í•œ ê³ ì • ID
    
    log_info(f"ğŸ¤– GPT ë¶„ì„ ì‹¤í–‰ ì¤‘... (ëŒ€ìƒ: {len(df)}ê±´)")
    try:
        df = analyze_articles_batch(df)
    except Exception as e:
        log_error(f"âŒ GPT ë¶„ì„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        if "ì¤‘ìš”ë„" not in df.columns: df["ì¤‘ìš”ë„"] = 0

    # 7. ì¤‘ìš”ë„ í•„í„°ë§ (ì¤‘ìš”ë„ 3ì  ì´ìƒ)
    if "ì¤‘ìš”ë„" in df.columns:
        df = df[df["ì¤‘ìš”ë„"] >= 3].reset_index(drop=True)
    
    log_info(f"âœ¨ í•„í„°ë§ ì™„ë£Œ: ìµœì¢… {len(df)}ê±´ ì„ ë³„ë¨")

    # 8. ì €ì¥
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    log_info(f"âœ… Step 1 ì €ì¥ ì™„ë£Œ: {output_file}")

if __name__ == "__main__":
    main()




# import os
# import sys
# import pandas as pd
# import html
# from datetime import datetime
# from utils.logger import log_info, log_error
# from utils.file_manager import get_today_folder, get_today_filename
# from utils.gpt_utils import analyze_articles_batch
# from utils.gpt_utils import deduplicate_news_with_gpt
# from utils.gpt_utils import deduplicate_news_with_gpt_twopass

# def _normalize_headline(s: pd.Series) -> pd.Series:
#     # ëŒ€ê´„í˜¸/ê³µë°±/ì¤‘ë³µê³µë°± ë“± ì •ê·œí™” (í´ë°± ì¤‘ë³µì œê±°ìš©)
#     return (
#         s.fillna("")
#          .str.replace(r"\[.*?\]", "", regex=True)
#          .str.replace(r"\s+", " ", regex=True)
#          .str.strip()
#          .str.lower()
#     )

# def _safe_twopass_dedupe(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     GPT 2íŒ¨ìŠ¤ ì¤‘ë³µì œê±°ë¥¼ ì•ˆì „í•˜ê²Œ ê°ì‹¸ëŠ” ë˜í¼.
#     - 1ì°¨: reset_index í›„ 2íŒ¨ìŠ¤ ì‹œë„
#     - 2ì°¨: ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ í•œ ë²ˆ ê°•ì œ reset í›„ ì¬ì‹œë„
#     - 3ì°¨(í´ë°±): í—¤ë“œë¼ì¸/URL ê¸°ë°˜ì˜ ì „í†µì  ì¤‘ë³µì œê±°
#     """
#     base = df.reset_index(drop=True).copy()
#     try:
#         return deduplicate_news_with_gpt_twopass(base.reset_index(drop=True))
#     except IndexError as e:
#         log_error(f"[WARN] GPT 2íŒ¨ìŠ¤ ì¤‘ë³µì œê±° IndexError 1ì°¨ ë°œìƒ: {e}. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
#         try:
#             return deduplicate_news_with_gpt_twopass(base.reset_index(drop=True))
#         except Exception as e2:
#             log_error(f"[FALLBACK] GPT 2íŒ¨ìŠ¤ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}. ê·œì¹™ ê¸°ë°˜ ì¤‘ë³µì œê±°ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
#             fb = base.copy()
#             # ê·œì¹™ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: URL ìš°ì„ , ê·¸ ë‹¤ìŒ ì •ê·œí™” í—¤ë“œë¼ì¸
#             fb["__norm_headline__"] = _normalize_headline(fb["í—¤ë“œë¼ì¸"])
#             fb = fb.drop_duplicates(subset=["URL"])
#             fb = fb.drop_duplicates(subset=["__norm_headline__"])
#             fb = fb.drop(columns=["__norm_headline__"])
#             fb = fb.reset_index(drop=True)
#             return fb

# def main():
#     log_info("ğŸ“„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
#     today_folder = get_today_folder()
#     input_file = os.path.join(today_folder, get_today_filename("step0_raw.csv"))
#     output_file = os.path.join(today_folder, get_today_filename("step1_filtered.csv"))

#     try:
#         df = pd.read_csv(input_file, encoding="utf-8-sig")
#     except Exception as e:
#         log_error(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
#         sys.exit(1)

#     # âš™ï¸ ì…ë ¥ ì•ˆì •í™”: URL ê²°ì¸¡ ì œê±°, ì—°ì† ì¸ë±ìŠ¤ + row_id ë¶€ì—¬
#     if "URL" not in df.columns:
#         log_error("âŒ ì…ë ¥ ë°ì´í„°ì— URL ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
#         sys.exit(1)

#     df = df[df["URL"].notna()].reset_index(drop=True).copy()
#     if "row_id" not in df.columns:
#         df["row_id"] = df.index  # 0..n-1 ê³ ì • ID

#     log_info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")

#     # âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í•œê¸€ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
#     domain_to_korean = {
#         "shindonga.donga.com": "ì›”ê°„ ì‹ ë™ì•„",
#         "magazine.hankyung.com": "ë§¤ê±°ì§„ í•œê²½",
#         "www.chosun.com": "ì¡°ì„ ì¼ë³´",
#         "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ",
#         "weekly.chosun.com": "ì£¼ê°„ì¡°ì„ ",
#         "www.joins.com": "ì¤‘ì•™ì¼ë³´",
#         "www.donga.com": "ë™ì•„ì¼ë³´",
#         "weekly.donga.com": "ì£¼ê°„ë™ì•„",
#         "www.khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
#         "weekly.khan.co.kr": "ì£¼ê°„ê²½í–¥",
#         "www.hani.co.kr": "í•œê²¨ë ˆ",
#         "www.hankyung.com": "í•œêµ­ê²½ì œ",
#         "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
#         "h21.hani.co.kr": "í•œê²¨ë ˆ21",
#         "insnews.co.kr": "í•œêµ­ë³´í—˜ì‹ ë¬¸",
#         "www.magazine.mk.co.kr": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
#         "www.hankookilbo.com": "í•œêµ­ì¼ë³´",
#         "view.asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
#         "www.edaily.co.kr": "ì´ë°ì¼ë¦¬",
#         "news.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
#         "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
#         "www.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
#         "www.sisain.co.kr": "ì‹œì‚¬IN",
#         "sports.khan.co.kr": "ìŠ¤í¬ì¸ ê²½í–¥",
#         "sports.donga.com": "ìŠ¤í¬ì¸ ë™ì•„",
#         "insweek.co.kr": "ë³´í—˜ì‹ ë³´",
#         "insjournal.co.kr": "ë³´í—˜ì €ë„",
#         "insnews.co.kr": "í•œêµ­ë³´í—˜ì‹ ë¬¸"
#     }
#     df["ë§¤ì²´ëª…"] = df["ë§¤ì²´ëª…"].map(domain_to_korean).fillna(df["ë§¤ì²´ëª…"])

#     # âœ… ì œëª© ì •ë¦¬: HTML ì œê±° + ëŒ€ê´„í˜¸ ì œê±°
#     df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].apply(html.unescape)
#     df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].str.replace(r"\[.*?\]", "", regex=True).str.strip()

#     # ====== â›³ï¸ ì¤‘ìš”: GPT ì¤‘ë³µ ì œê±°(ì•ˆì „ ë˜í¼ ì‚¬ìš©) ======
#     # ë‚´ë¶€ì—ì„œ ë°°ì¹˜ ìŠ¬ë¼ì´ì‹± ì‹œ iloc ë²”ìœ„ ë¬¸ì œê°€ ìƒê¸°ì§€ ì•Šë„ë¡ ì…ë ¥ì„ í•­ìƒ ì—°ì† ì¸ë±ìŠ¤ë¡œ ì œê³µ
#     df = df.reset_index(drop=True)
#     df = _safe_twopass_dedupe(df)
#     df = df.reset_index(drop=True)

#     # ====== GPT ë¶„ì„ ì‹¤í–‰ ì „ì—ë„ ì¸ë±ìŠ¤ ì •ë¦¬ ======
#     df = df.reset_index(drop=True)
#     df = analyze_articles_batch(df)

#     # âœ… ì¤‘ìš”ë„ 3 ì´ìƒë§Œ í•„í„°ë§
#     if "ì¤‘ìš”ë„" not in df.columns:
#         log_error("âŒ GPT ë¶„ì„ ê²°ê³¼ì— 'ì¤‘ìš”ë„' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
#         sys.exit(1)

#     df = df[df["ì¤‘ìš”ë„"] >= 3].reset_index(drop=True)
#     log_info(f"âœ¨ ì¤‘ìš”ë„ 3 ì´ìƒ ê¸°ì‚¬ ìˆ˜: {len(df)}ê±´")

#     # âœ… ì €ì¥
#     os.makedirs(today_folder, exist_ok=True)
#     df.to_csv(output_file, index=False, encoding="utf-8-sig")
#     log_info(f"âœ… ì¤‘ìš” ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: {output_file}")

# if __name__ == "__main__":
#     main()
