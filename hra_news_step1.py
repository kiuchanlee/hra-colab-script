import pandas as pd
import html
import re
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

# ë³¸ë¬¸ ìŠ¤í¬ë©í•‘ í•¨ìˆ˜
def get_naver_news_body(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
    except Exception as e:
        return f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}"

    if response.status_code != 200:
        return f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}"

    soup = BeautifulSoup(response.text, 'html.parser')

    content = soup.find('article', {'id': 'dic_area'})
    if not content:
        return "âŒ ë³¸ë¬¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"

    text = content.get_text(separator="\n", strip=True)
    return text

def main():
    print("ğŸ“¥ ë‰´ìŠ¤ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    try:
        df = pd.read_csv("crawled_news.csv", encoding="utf-8-sig")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    # 1. tqdm ì§„í–‰ë°” ì ìš©
    tqdm.pandas()

    # 2. ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í•œê¸€ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
    domain_to_korean = {
        "www.chosun.com": "ì¡°ì„ ì¼ë³´",
        "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ",
        "weekly.chosun.com": "ì£¼ê°„ì¡°ì„ ",
        "www.joins.com": "ì¤‘ì•™ì¼ë³´",
        "www.donga.com": "ë™ì•„ì¼ë³´",
        "weekly.donga.com": "ì£¼ê°„ë™ì•„",
        "www.khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
        "weekly.khan.co.kr": "ì£¼ê°„ê²½í–¥",
        "www.hani.co.kr": "í•œê²¨ë ˆ",
        "www.hankyung.com": "í•œêµ­ê²½ì œ",
        "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "www.magazine.mk.co.kr": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
        "www.hankookilbo.com": "í•œêµ­ì¼ë³´",
        "view.asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
        "www.edaily.co.kr": "ì´ë°ì¼ë¦¬",
        "news.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
        "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "news.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
        "www.sisain.co.kr": "ì‹œì‚¬IN"
    }
    df["ë§¤ì²´ëª…"] = df["ë§¤ì²´ëª…"].map(domain_to_korean).fillna(df["ë§¤ì²´ëª…"])

    # 3. ì œëª© HTML ë””ì½”ë”© + [ëŒ€ê´„í˜¸] ì œê±°
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].apply(html.unescape)
    df["í—¤ë“œë¼ì¸"] = df["í—¤ë“œë¼ì¸"].str.replace(r"\[.*?\]", "", regex=True).str.strip()

    # 4. ë³¸ë¬¸ ìˆ˜ì§‘
    print("ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    df["ë³¸ë¬¸"] = df["URL"].progress_apply(get_naver_news_body)

    # 5. ì¸ë±ìŠ¤ ì´ˆê¸°í™” í›„ ì €ì¥
    df = df.reset_index(drop=True)
    df.to_csv("crawled_news.csv", index=False, encoding="utf-8-sig")
    print("âœ… ì €ì¥ ì™„ë£Œ: crawled_news.csv")

if __name__ == "__main__":
    main()
