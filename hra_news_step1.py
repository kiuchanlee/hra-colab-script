
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime, timedelta
from tqdm import tqdm
import math

# âœ… êµ¬ê¸€ ì¸ì¦ ë° gspread ì„¤ì • (GitHub Actionsìš©)
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials

def authenticate_gspread():
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]
    creds = Credentials.from_service_account_file("creds.json", scopes=scopes)
    return gspread.authorize(creds)

gc = authenticate_gspread()

# âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news(query, category, start_date, end_date, max_page=1):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.naver.com"
    }

    media_codes = ",".join([
        "023", "025", "020", "032", "028", "015", "009",
        "081", "005", "008", "014", "016", "018", "277", "001"
    ])

    results = []
    seen_links = set()

    for start in range(1, max_page * 10 + 1, 10):
        print(f"\nğŸ“„ [{category} - {query}] í˜ì´ì§€ {((start - 1)//10) + 1} í¬ë¡¤ë§ ì¤‘...")

        url = (
        f"https://search.naver.com/search.naver?where=news&query={query}"
        f"&pd=4&ds={start_date}&de={end_date}&office_type=1&office_section_code=1"
        f"&sort=0&start={start}"
        )

        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            page_links = list({a["href"] for a in soup.select("a.info") if "n.news.naver.com" in a["href"]})

            print(f"ğŸ”— ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ ìˆ˜: {len(page_links)}")

            for link in page_links:
                if link in seen_links:
                    continue
                seen_links.add(link)

                try:
                    article = requests.get(link, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article.text, "html.parser")

                    content = article_soup.select_one("div#newsct_article")
                    content_text = content.get_text(separator=" ").strip() if content else ""

                    title_tag = article_soup.select_one("h2#title_area span")
                    title = title_tag.get_text().strip() if title_tag else ""

                    press_tag = article_soup.select_one("img.media_end_head_top_logo_img")
                    press = press_tag['alt'].strip() if press_tag and 'alt' in press_tag.attrs else ""

                    date_tag = article_soup.select_one("span.media_end_head_info_datestamp_time")
                    raw_date = date_tag.get_text().strip() if date_tag else ""

                    try:
                        raw_date_fixed = raw_date.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM")
                        dt = datetime.strptime(raw_date_fixed, "%Y.%m.%d. %p %I:%M")
                        weekday_kor = ["ì¼", "ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† "][(dt.weekday() + 1) % 7]
                        formatted_date = dt.strftime(f"%m.%d({weekday_kor})")
                    except Exception:
                        formatted_date = raw_date

                    results.append({
                        "êµ¬ë¶„": category,
                        "í‚¤ì›Œë“œ": query,
                        "ì¼ì": formatted_date,
                        "í—¤ë“œë¼ì¸": title,
                        "ë³¸ë¬¸": content_text,
                        "ë§¤ì²´ëª…": press,
                        "URL": link
                    })

                    print(f"âœ… [{formatted_date}] [{press}] {title[:30]}...")
                    time.sleep(1)

                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {link} - {e}")
                    continue

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

    return pd.DataFrame(results)

# âœ… í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜
keywordGroups =  [ {
        "category": "HR",
         "keywords": [
    # ğŸ¢ ì¸ì‚¬ ì „ëµ (HR Strategy)
    # â–ª ì¡°ì§ êµ¬ì¡° ë° ì¸ì‚¬ ì œë„
    "ì¡°ì§ê°œí¸", "ì¸ì‚¬ì œë„", "ì§ë¬´ì¤‘ì‹¬ ì¸ì‚¬", "ì§ë¬´ê¸‰ì œ", "ì„±ê³¼ê¸‰ì œ", "í˜¸ë´‰ì œ ê°œí¸", "ì§ê¸‰ì²´ê³„"

    # â–ª ì±„ìš© ë° ì¸ì¬ í™•ë³´
    ,"ì±„ìš©ê³µê³ ", "ì‹ ì…ê³µì±„", "ì»¬ì²˜í•", "ì»¬ì³í•", "ì‹ ì…ì±„ìš©", "ê²½ë ¥ì±„ìš©", "ì¸í„´ì±„ìš©",
    "ì±„ìš©ì‹œì¥", "ë¸”ë¼ì¸ë“œì±„ìš©", "ì±„ìš©ê³µì •ì„±", "ê³ ìš©ì‹œì¥", "ê²½ë ¥ë‹¨ì ˆ", "ì±„ìš©ë©´ì ‘",
    "ê³µì±„ íì§€", "ìƒì‹œì±„ìš©", "ì¸ì¬ í™•ë³´", "ë¦¬í¬ë£¨íŒ…", "ì±„ìš© ì „ëµ", "ìˆ˜ì‹œì±„ìš©", "ê¸€ë¡œë²Œ ì¸ì¬"

    # â–ª ì¸ì‚¬í‰ê°€ ë° ë³´ìƒ
    "ì¸ì‚¬í‰ê°€", "ì„±ê³¼í‰ê°€", "ì—­ëŸ‰í‰ê°€", "ë³´ìƒì²´ê³„", "ì—°ë´‰ì œ", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ",
    "ì„ê¸ˆì¸ìƒ", "ì„ê¸ˆí˜‘ìƒ", "ê¸°ë³¸ê¸‰", "ì—°ì°¨ìˆ˜ë‹¹", "í†µìƒì„ê¸ˆ", "ì„ê¸ˆí”¼í¬",
    "ì„ê¸ˆì²´ê³„", "ì‹œê¸‰ì œ", "ìµœì €ì„ê¸ˆ", "ê³ ì •ê¸‰", "ì„±ê³¼ì—°ë´‰", "í‡´ì§ê¸ˆ",
    "í‰ê· ì—°ë´‰", "ê·¼ì†", "í¬ê´„ì„ê¸ˆ", "ì‹¤ì—…ê¸‰ì—¬", "ì—°ê¸ˆê°œí˜","ìŠ¤í†¡ì˜µì…˜","RSU",

    # ğŸ’¼ ì¸ì¬ ê°œë°œ (Talent Development)
    # â–ª êµìœ¡ ë° ë¦¬ìŠ¤í‚¬ë§
    "ì‚¬ë‚´ êµìœ¡", "ë¦¬ìŠ¤í‚¬ë§", "ì—…ìŠ¤í‚¬ë§", "êµìœ¡í›ˆë ¨", "HRD", "ì‚¬ë‚´ëŒ€í•™","ì¡í¬ìŠ¤íŒ…", "ì‚¬ë‚´ ì–‘ì„±",

    # â–ª ê²½ë ¥ ê°œë°œ ë° ìŠ¹ì§„
    "ì»¤ë¦¬ì–´íŒ¨ìŠ¤", "ê²½ë ¥ê°œë°œ", "ì§ë¬´ìˆœí™˜", "ìŠ¹ì§„ì œë„", "ë¦¬ë”ì‹­ í”„ë¡œê·¸ë¨", "í›„ê³„ì ì–‘ì„±", "ì „ì§",

    # ğŸ§‘â€ğŸ’» ê·¼ë¬´ í™˜ê²½ ë° ì¡°ì§ ë¬¸í™” (Workplace & Culture)
    # â–ª ê·¼ë¬´ì œë„ ë° ìœ ì—°ê·¼ë¬´
    "52ì‹œê°„", "ìœ ì—°ê·¼ë¡œ", "ì¬íƒê·¼ë¬´", "ìœ ì—°ê·¼ë¬´ì œ",
    "ì„ íƒê·¼ë¡œì œ", "ê·¼ë¡œì‹œê°„ ë‹¨ì¶•", "ì£¼4ì¼ì œ", "ë‹¨ì¶•ê·¼ë¡œ", "ê·¼ë¬´ì‹œê°„", "ìœ¡ì•„íœ´ì§", "íœ´ì§ì œë„", "ì¶œì‚°íœ´ê°€", "ê·¼ì†íœ´ê°€", "ì‚¬ë‚´ë³µì§€",

    # â–ª ì¡°ì§ë¬¸í™” ë° ë‹¤ì–‘ì„±
    "ì¡°ì§ë¬¸í™”", "ì›Œë¼ë°¸", "DEI", "ì‚¬ë‚´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",

    # âš–ï¸ ì •ì±… ë° ë…¸ë™ë²• (HR Policy & Labor Law)
    # â–ª ì •ë¶€ ì •ì±… ë° ì œë„ ë³€í™”
    "ì¸ì‚¬ ì •ì±…", "ê³ ìš© ì •ì±…", "ë…¸ë™ì‹œì¥ ê°œí¸", "ì¸ì‚¬ í–‰ì •", "ì¸ì¬ ì •ì±…",

    # â–ª ë…¸ë™ë²• ë° ê·œì œ
    "ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ë²•", "ë…¸ë™ì‹œê°„", "ì¤‘ëŒ€ì¬í•´ë²•",
    "ì§ì¥ ë‚´ ê´´ë¡­í˜", "ê³ ìš© ì•ˆì •ì„±", "ì‚°ì—…ì¬í•´", "ë…¸ì¡°í˜‘ì•½", "ì„ë‹¨í˜‘", "ë‹¨ì²´êµì„­",
    "ê·¼ë¡œí™˜ê²½", "ê·¼ë¡œê³„ì•½", "ì •ë…„ì—°ì¥", "íŒŒì—…", "ì‹¤ì—…ì", "ë…¸ì¡°í™œë™",
    "ë…¸ì‚¬", "ë…¸ì‚¬ê°ˆë“±", "êµ¬ì¡°ì¡°ì •", "ì„ê¸ˆí˜‘ìƒ", "í¬ë§í‡´ì§",

    # ğŸ“Š HR í…Œí¬ ë° ë°ì´í„° (HR Tech & Analytics)
    # â–ª HR í…Œí¬ ë„ì… ì‚¬ë¡€
    "HR Tech", "AI ì±„ìš©", "HR ìë™í™”", "ì¸ì‚¬ê´€ë¦¬ ì‹œìŠ¤í…œ", "HRIS",

    # â–ª ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬
    "ì¸ì‚¬ ë°ì´í„°", "HR Analytics", "ì¸ì‚¬ ë°ì´í„° ë¶„ì„",
    "ë°ì´í„° ê¸°ë°˜ í‰ê°€",

    # ğŸ¢ ê·¸ë£¹/ê¸°ì—…ë³„ ì¸ì‚¬ í‚¤ì›Œë“œ (ëŒ€ê¸°ì—… ê´€ë ¨)
    "ì‚¼ì„± ì¸ì‚¬", "ì‚¼ì„± ì„ì›ì§„", "ì‚¼ì„± ì¡°ì§", "ì‚¼ì„± ê²½ì˜ì§„", "ì‚¼ì„± ì¡°ì§ê°œí¸", "ì‚¼ì„± ì˜ì…", "ì‚¼ì„± ë‚´ì •", "ì‚¼ì„± ë°œë ¹", "ì‚¼ì„± ì‚¬ì™¸ì´ì‚¬", "ì‚¼ì„± ë³µê·€"
    "í˜„ëŒ€í•´ìƒ ì¸ì‚¬", "í˜„ëŒ€í•´ìƒ ì„ì›ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§", "í˜„ëŒ€í•´ìƒ ê²½ì˜ì§„", "í˜„ëŒ€í•´ìƒ ì¡°ì§ê°œí¸", "í˜„ëŒ€í•´ìƒ ì˜ì…", "í˜„ëŒ€í•´ìƒ ë‚´ì •", "í˜„ëŒ€í•´ìƒ ë°œë ¹", "í˜„ëŒ€í•´ìƒ ì‚¬ì™¸ì´ì‚¬",
    "DBì†ë³´ ì¸ì‚¬", "DBì†ë³´ ì„ì›ì§„", "DBì†ë³´ ì¡°ì§", "DBì†ë³´ ê²½ì˜ì§„", "DBì†ë³´ ì¡°ì§ê°œí¸", "DBì†ë³´ ì˜ì…", "DBì†ë³´ ë‚´ì •", "DBì†ë³´ ë°œë ¹", "DBì†ë³´ ì‚¬ì™¸ì´ì‚¬",
    "KBì†ë³´ ì¸ì‚¬", "KBì†ë³´ ì„ì›ì§„", "KBì†ë³´ ì¡°ì§", "KBì†ë³´ ê²½ì˜ì§„", "KBì†ë³´ ì¡°ì§ê°œí¸", "KBì†ë³´ ì˜ì…", "KBì†ë³´ ë‚´ì •", "KBì†ë³´ ë°œë ¹", "KBì†ë³´ ì‚¬ì™¸ì´ì‚¬",
    "ë©”ë¦¬ì¸  ì¸ì‚¬", "ë©”ë¦¬ì¸  ì„ì›ì§„", "ë©”ë¦¬ì¸  ì¡°ì§", "ë©”ë¦¬ì¸  ê²½ì˜ì§„", "ë©”ë¦¬ì¸  ì¡°ì§ê°œí¸", "ë©”ë¦¬ì¸  ì˜ì…", "ë©”ë¦¬ì¸  ë‚´ì •", "ë©”ë¦¬ì¸  ë°œë ¹", "ë©”ë¦¬ì¸  ì‚¬ì™¸ì´ì‚¬"
]}]

# âœ… ë‚ ì§œ ì„¤ì •
print("\nğŸ“† ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •")
days_ago = 1
end_date = datetime.today().strftime("%Y.%m.%d")
start_date = (datetime.today() - timedelta(days=days_ago)).strftime("%Y.%m.%d")
print(f"ğŸ“† ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n")

# âœ… ì „ì²´ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹¤í–‰
all_results = []
for group in keywordGroups:
    for keyword in group["keywords"]:
        df = crawl_news(keyword, group["category"], start_date, end_date, max_page=1)
        all_results.append(df)

media_codes = [
    # ğŸ—ï¸ ì£¼ìš”ì¼ê°„ì§€
    "023",  # ì¡°ì„ ì¼ë³´
    "025",  # ì¤‘ì•™ì¼ë³´
    "020",  # ë™ì•„ì¼ë³´
    "032",  # ê²½í–¥ì‹ ë¬¸
    "028",  # í•œê²¨ë ˆ
    "015",  # í•œêµ­ê²½ì œ
    "009",  # ë§¤ì¼ê²½ì œ
    "001",  # ì—°í•©ë‰´ìŠ¤
    "469",  # í•œêµ­ì¼ë³´


    # ğŸ’° ê²½ì œ ì „ë¬¸ì§€/ê²½ì œ ë§¤ì²´
    "366",  # ì¡°ì„ ë¹„ì¦ˆ
    "277",  # ì•„ì‹œì•„ê²½ì œ
    "243",  # ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸ 
    "018", # ì´ë°ì¼ë¦¬ 
    "016",  # í—¤ëŸ´ë“œê²½ì œ
    "014",  # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
    "008",  # ë¨¸ë‹ˆíˆ¬ë°ì´

    # ğŸ“° ê²½ì œ ì£¼ê°„ì§€ / ì‹œì‚¬ì£¼ê°„ì§€(ê²½ì œ ì¤‘ì‹¬)
    "024",  # ë§¤ê²½ì´ì½”ë…¸ë¯¸
    "050",  # í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤
    "037",  # ì£¼ê°„ë™ì•„
    "308",  # ì‹œì‚¬IN
    "051",  # ì£¼ê°„ì¡°ì„ 
]

# âœ… ë°ì´í„° ì •ë¦¬
df_total = pd.concat(all_results, ignore_index=True)
df_total = df_total.drop_duplicates(subset=["URL"])
# 3. article/ë‹¤ìŒ ì„¸ìë¦¬ ìˆ«ì ì¶”ì¶œ
df_total["media_code"] = df_total["URL"].str.extract(r'article/(\d{3})/')
df_total = df_total[df_total["media_code"].isin(media_codes)]
df_total = df_total[["êµ¬ë¶„", "í‚¤ì›Œë“œ", "ì¼ì", "í—¤ë“œë¼ì¸", "ë³¸ë¬¸", "ë§¤ì²´ëª…", "URL"]]
df_total['í—¤ë“œë¼ì¸'] = df_total['í—¤ë“œë¼ì¸'].str.replace(r"\[.*?\]", "", regex=True).str.strip()
df_total = df_total.sort_values(by=["êµ¬ë¶„", "ì¼ì", "í—¤ë“œë¼ì¸"], ascending=[True, False, True])
df_total.to_csv("crawled_news.csv", index=False)
print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ ë° csv ì €ì¥!")
